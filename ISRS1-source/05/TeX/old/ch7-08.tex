We begin an investigation of relations between two numerical variables. For instance, perhaps we would like to look at the relationship between \_\_\_ and \_\_\_. We might notice that if \_\_\_ increases, then \_\_\_ tends to also increase \textit{on the average}. We wish to quantify this relationship and fit linear models to this data. We begin by exploring plots and some important properties to fitting a line. We then discuss how to find what might be called the ``best'' linear fit and its properties. \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scatterplots, residuals, and line fitting}

Suppose we are given a set of data where each point may be plotted on an x-y plane. If the variables can be categorized as explanatory and response, the response typically is used as the y-variable and the explanatory as the x-variable. Such a plot is a \textbf{scatterplot}. Each axis represents a different variable, perhaps one being height and weight of players from a basketball team.

[x-y plot of the data]

In such a set of data, we might notice some association or trend within the data. In this case [ discuss ]. Perhaps tempted to go further than just stating this tendency, fitting a line to the data seems appropriate. This line will also be called the \textbf{model} or \textbf{fit}, and we restrict our attention to linear fits. Below are three different lines we might fit to the data.

[3 plots with possible fits]

The first two lines appear to fit the data well whereas the third does not tend to follow the data very well. There is a tendency of the first lines to be close to the data points, making it give a more satisfactory fit. This seems aesthetically proper but this needs to be quantified. Furthermore, a criteria is needed to determine which of two particular lines has a ``better'' fit. \\

Looking at the vertical distance of each point from the line is one possible tool for setting up a criteria. Take observation $i$, $(x_i,y_i)$, and our line, then we can find this vertical distance easily if we know the the y-value of the line at the x-value $x_i$; this y-value is typically labeled $\hat{y}_i$ and is the \textbf{expected} or \textbf{predicted} response from the linear fit. The \textbf{residual} of point $i$ is given by
\begin{eqnarray*}
r_i = \text{actual} - \text{expected} = y_i - \hat{y}_i
\end{eqnarray*}
This is exactly the vertical distance between the true observed value and the value that we would predict if provided only $x_i$ and the model. Notice that if the observed value is below the line, the residual is negative, and if it is above, the residual is positive. If this absolute value of the residual, $|r_i|$, is small, then one might say that the model predicted the point well. If $|r_i|$ is large, then the model poorly predicted the point. \\

[2 plots, one showing a positive residual and one showing a negative residual] \\

We seek to look at the residuals of all points. If the residuals as a whole tend to be small, then the fit is good relative to a fit where they tend to be larger. One possible criterion to determine how well a line fits data is the sum of these residual distances (in absolute value): $\sum_{i=1}^n |r_i|$. If one fit has a smaller value for the sum, it would be said to be better. \\

Although the sum of absolute residuals is a good method, it is not typically the best, and it also misses another important consideration. Linear models are often chosen to help in prediction, in which case predictions that are very far off are exceedingly problematic. The sum of absolute residuals doesn't take this into account, at least not as much as is usually desirable. To solve this problem, take the residuals, square their values, and then take the sum. This is called the \textbf{residual sum of squares (RSS)}:
\begin{eqnarray*}
RSS = \sum_{i=1}^n r_i^2
\end{eqnarray*}
Using this summation, if a residual is large, the RSS is punished accordingly; squaring a large number results in a very large number. Picking models based on reducing RSS is very common and is often preferable to the sum of absolute residuals. \\

Using this criteria, we can compare the two fits in figure 6.\_. Because the ... it is the better fit. \\

Comparing many lines to eventually get something close to the best fit is one method to get a very good model. Compare two lines, pick the best, and compare this one with another, pick the best, and so on. But is there a better way than this iterative process? Thankfully there is; one particular line exists that will be declared better than any comparing line. This line is called the \textbf{least squares residual line (LSR line)}. The LSR line has the \textit{minimum RSS}, meaning that every other line's RSS will be larger than that of the LSR line. In section 6.3, we will determine how to find this line but first some additional background is important. \\

%Let's first focus on plotting data before we discussion the relationship between two variables. Below is a sample of a scatterplot. Each point represents one subject in the sample, where the x-value of the point represents the measure 
% par(mar=c(4,4,2,2),las=1); x <- 0:20; y <- abs(x + rnorm(20,6,3)); plot(x,y,axes=F,xlab='X',ylab='Y'); axis(1); axis(2)

In chapter 1, we discussed associations between two variables, where information regarding one variable provides useful information about the other variable. For two numerical variables, we can plot the points in a \textbf{scatterplot}. If there is an association in the data, then we may notice trends or tendencies in one or more portions of the scatterplot. Below are some examples where there is an association. 
\begin{figure}[htp]
\centering
\includegraphics[height=1.35in]{types-of-association1.pdf}
\caption{Four forms of association. In each case, knowing the x-value is useful in estimating where the y-value will fall.} %The first plot shows a positive association, the second negative association, and the third an association that cannot be classified as positive or negative.}
\end{figure}
Although there are many statistical methods to address associations, we only focus on linear associations, or associations where the points approximate a straight line. Of the four plots above, only the first two have linear trends that are appropriate for our model fitting. The third plot has a clearly non-linear trend, and the fourth plot appears to be constant for small and large values of $x$ but random for central values. \\

What is the best fitting straight line to the first two plots above? Although we will determine the LSR line exactly next section, we can say now with confidence that it would have positive slope in the first plot and negative slope in the second. In these cases, we could say that there is a \textbf{positive} and \textbf{negative association}, respectively. This linear trend upwards or downwards corresponds to a measure of \textbf{correlation}. Correlation, labeled $r$, is a numerical value falling between $-1$ and $+1$. The sign of $r$ corresponds to whether the LSR line will have positive or negative slope, and the closer $r$ is to $-1$ or $+1$, the stronger the linear relationship in the data. For the correlation to be exactly -1 or 1, the data points must fall precisely on a straight line. A correlation of 0 means the best fitting line has slope 0; it is a flat line. \\

Below are four plots of successively stronger (positive) linear relationships. The correlation increases from approximately 0 to nearly 1.
\begin{figure}[htp]
\centering
\includegraphics[height=1.35in]{strength-of-association.pdf}
\caption{Four plots with different levels of association. The first has no discernible association.} %The first plot shows a positive association, the second negative association, and the third an association that cannot be classified as positive or negative.}
\end{figure}

To compute the correlation, we use a formula that captures the linear trend:
\begin{eqnarray*}
r	&=& \frac{1}{n-1}\sum_{i=1}^n \frac{x_i - \bar{x}}{s_x}\frac{y_i - \bar{y}}{s_y} \\
	&=& \frac{1}{(n-1)s_x s_y}\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})
\end{eqnarray*}
Here $n$ is the number of data point pairs and $(x_i,y_i)$ represents the $i^{th}$ data point. Putting the steps of this computation into words,
\begin{enumerate}
\item Compute the mean and standard deviation for the $x$'s and $y$'s separately: $\bar{x},\bar{y},s_x,s_y$.
\item For each data point $(x_i,y_i)$, compute $(x_i-\bar{x})(y_i-\bar{y})$. Add these up.
\item Divide the resulting sum by $(n-1)s_xs_y$.
\end{enumerate}

\begin{example}
[Do an example.]
\end{example}

What happens when we determine the correlation of a non-linear trend? Consider the third plot in figure 6.\_\footnote{Such a plot may have arise when considering the number of people in a park and the daily high temperature (very cold and very hot days will have virtually no visitors).}. There is no overall trend up or down, so the best straight line to fit the data would be flat. Thus, even if there is a strong non-linear trend, the correlation may not detect it, which suggests it is always important to view a plot of data before fitting models. \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least squares regression}

With the tool of correlation and the concepts of residuals and the least squares regression line, the best fit is within reach. Some last minute assumptions must be mentioned that help ensure that the residual sum of squares (RSS) is indeed the best criteria to use and that the line accurately reflects the data:
\begin{itemize}
\item Each data point is independent of the others. For instance, we would not use several successive days for the daily temperature; if one day is abnormally warm, it is likely that the days surrounding it will be above average.
\item The trend is linear. It would be inappropriate to fit a linear model to data that is clearly not linear.
\item The residuals are normally distributed. This may be checked via a normal quantile plot\footnote{A test that may also be used in some computer programs is the Shapiro-Wilks test. This will be introduced in chapter \_\_.}.
\end{itemize}
If these assumptions hold, fitting a line using the methods below is appropriate; regression methods used when these assumptions are not met will not be address. For brevity, we omit the computations necessary to derive properties and determine the LSR line, and we present only the results. There are three important properties of the LSR line. It is known that the LSR line
\begin{itemize}
\item has slope $r\frac{s_y}{s_x}$,
\item intersects the point $(\bar{x},\bar{y})$, and
\item has residuals that sum to zero: $\sum_{i=1}^nr_i = 0$.
\end{itemize}
Using the first two properties, an equation for the LSR line may be determined using the following steps:
\begin{enumerate}
\item Find $\bar{x}$, $\bar{y}$, $s_x$, $s_y$, and $r$.
\item Compute the slope $b_1 = r\frac{s_y}{s_x}$.
\item Because $(\bar{x},\bar{y})$ is on the line to find the equation of the line, the equation of the line is
	\begin{eqnarray*}
	y - \bar{y} = b_1(x - \bar{x})
	\end{eqnarray*}
\end{enumerate}
The form of the equation above is point-slope. We would like everything except $y$ on the right hand side, like below:
\begin{eqnarray*}
\hat{y} = b_0 + b_1x,\text{ where }b_0 = \bar{y} - b_1\bar{x}
\end{eqnarray*}
The ``hat'' is added to $y$ to imply that this equation estimates or predicts the y-value but does not give the true y-value. Thus, given an explanatory variable, $x_0$, the equation above may be used to predict the response variable, $y_0$. This prediction will not be perfect, of course, but approximates gives the best estimate of where $y_0$ will fall.
\begin{figure}[htp]
\centering
\includegraphics[height=2.1in]{r2-plot.pdf}
\caption{The rotated histogram represents the y-values and variability of a naive guess not using the model. The rotated red distribution represents the distribution of $y$ about $\hat{y}$ when using the model and $x$.} %The first plot shows a positive association, the second negative association, and the third an association that cannot be classified as positive or negative.}
\end{figure}

Worth mention is the \textbf{r-squared} of a model (the correlation squared: $r^2$). It is said that the model explains $r^2$ of the original variability in the response. What does this mean? Picking an individual at random, we have two cases: use the model and the explanatory $x$ or not. Without the model, the variability (variance) of the best guess at the response, $\bar{y}$, is approximated by $s_y^2$. However, utilizing the model gives an informed estimate, $\hat{y}$, that is typically a better guess than $\bar{y}$. It can be shown that its variance of $y$ about $\hat{y}$ is $(1-r^2)s_y^2$. That is, the variability is reduced by a factor $r^2$ when the model is taken into account. This is shown graphically in figure ZZQ. Thus a large $r^2$ (close to 1) represents a better fitting LSR line. \\

\begin{example}
Provide statistics and setup... \\
(a) Determine the LSR line of ... \\
(b) Given this LSR fit, predict ... if ... \\
(c) If the actual value is... \\
(d) What amount of variation in the y-data is explained y the x-data?
\end{example}

\begin{example}
Provide statistics and setup... \\
(a) Determine the LSR line of ... \\
(b) Given this LSR fit, predict ... if ... \\
(c) If the actual value is... \\
(d) What amount of variation in the y-data is explained y the x-data?
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Verifying assumptions} 

The three assumptions made when computing a LSR line cannot be taken for granted and must be checked. This may be done visually or systematically. For brevity and to reduce complexity, only the visual approaches will be used here, which focus on using residual plots and normal-quantile plots. A \textbf{residual plot} graphs the residuals, which is useful to verify observation independence and linearity. \\

Verifying the observations are independent primarily focuses on whether one observation tends to be close to the last. For instance, if the daily high temperature is tracked from April to June, then if April 12th appears to be unusually warm, then it is likely that April 11th and 13th are also above average. In other words, consecutive observations may be correlated. Plotting the residuals by their order of collection allows these tendencies to become apparent. If the data points appear randomly scattered, i.e. the residuals appear to be independent of one another, then the independence between observations is reasonable. Figure \_\_ shows a case where the observations do not appear to be independent. \\ % Depending on the problem, it may also be appropriate to construct a residual plot using an order other than the order of collection.

[Figure \_\_ showing two (one?) residual plot(s).] \\

Linearity is a key assumption in the construction of the LSR line. If the line is not linear, then a linear fit is inappropriate. To verify linearity, plot each residual, $(x_i, r_i) = (x_i, y_i - \hat{y}_i)$. Similar to when checking for observation independence, there should be independence between consecutive observations. \\

To verify the data are normally distributed, a normal quantile plot of the residuals may be used. By assumption, the residuals are normally distributed, implying this plot will be approximately linear. It is especially important to look for strong deviations at the tails of the normal quantile plot; observations with abnormally large residuals may be very influential on the fit. \\

[Figure \_\_ showing a normal quantile plot that is not normal.] \\

There are methods to fit models to data that do (does?) not follow the three assumptions, although they are beyond the scope of this work. \\