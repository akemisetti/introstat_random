%\begin{example}{Having checked the integrity of the final model (coefficient summary in Table~\ref{outputForMultipleRegrOutputForAllPredictorsButDurationAndStockPhoto} on page~\pageref{outputForMultipleRegrOutputForAllPredictorsButDurationAndStockPhoto}), we might like to interpret the model. A good starting point is to construct a confidence interval for each parameter and describe what each parameter means in context. Here we provide a confidence interval for $\beta_1$, the coefficient of \var{condNew}, and a simple explanation for the meaning of $\beta_1$.}
%The point estimate and standard error for $\beta_1$, the coefficient of \var{condNew}, are 5.585 and 0.925, and the degrees of freedom associated with the model are $df=138$. For a 95\% confidence interval, we can use $t^*_{100} = 1.98$ from Appendix~\ref{tDistributionTable} ($t^*_{138}$ is not available, so we use the next smaller $df$ listed). Then the 95\% confidence interval for $\beta_1$ is
%\begin{align*}
%5.585 \pm 1.98*0.925 \quad\to\quad (3.75, 7.42)
%\end{align*}
%The $\beta_1$ coefficient represents the premium paid for a new game on Ebay versus a used game. We are 95\% confident people pay between \$3.75 and \$7.42 more for a new game than a used game \emph{on the average}.
%\end{example}

%\begin{exercise}
%Compute a confidence interval for $\beta_4$, the coefficient for the wheels variable, and interpret the coefficient in context. Answer in the footnote\footnote{95\% CI: (6.16, 8.31). The $\beta_4$ coefficient represents the estimated cost of each additional Wii wheel. For example, the model estimates that an auction with two Wii wheels would cost about $2*7.42=\$14.84$ more than an auction that included just the game.}.
%\end{exercise}


%%%%%%
%\section{Transforming variables in a linear model}
%
%When we observe a nonlinear relationship between a predictor and outcome variable, we can sometimes try transformations on either the predictor or the response variable to make the relationship more linear. Generally we only attempt such transformations when a linear model is inadequate.
%
%\begin{caution}
%{Avoid transformations when possible}
%{First, limit the use of transformations. Second, don't try transformations on any variables simply because those variables were eliminated in the model selection step.}
%\end{caution}
%
%Modeling provides a way to interpret the relationship between variables. When many of the variables are transformed, that relationship becomes more difficult to understand and justify. For these reasons, it is helpful to limit the use of  transformations.
%
%Transformations that might be considered on predictor variables include the following:
%\begin{description}
%\item[Natural logarithm.] If a few of observations are many times larger than the others (e.g. some observations are in the hundreds while most others are below 10), a natural logarithm may help restrain the impact of such large values. Note that the natural logarithm requires that all observations are greater than zero.
%\item[Square root.] The square root is a helpful alternative to the natural logarithm when using count data or when observations take value zero.
%\item[Squaring a predictor.] If there is curvature in the residuals against a predictor variable, then consider incorporating an additional variable into the model: the square of the predictor.
%\end{description}







%%%%
% Regression output for categorical variables.

%Recall that the underlying model that we are testing takes the following form:
%\begin{align*}
%x_{ij} = \mu_i + \epsilon_{j}
%\end{align*}
%This model is a modified version of the multiple regression equation, and we can actually obtain a regression summary for the model in addition to the ANOVA summary, as shown in Table~\ref{regressionSummaryTableForOBPAgainstPosition}. There are a few peculiarities in a regression summary that includes a categorical variable. First, notice that one of the levels is missing: \resp{C}. The catcher group has been used as the \term{baseline group}; in this case, the software selected the catcher group because C comes before the DH, IF, and OF when the labels were alphabetized. The baseline group is represented by the intercept. The sample means can be identified using the regression table, though we should be cautious in using the standard errors and p-values.
%\begin{table}[ht]
%\centering
%\begin{tabular}{rrrrr}
%  \hline
% & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
%  \hline
%(Intercept) & 0.3226 & 0.0057 & 56.69 & 0.0000 \\ 
% positionDH & 0.0252 & 0.0111 & 2.27 & 0.0237 \\ 
% positionIF & 0.0089 & 0.0064 & 1.40 & 0.1629 \\ 
% positionOF & 0.0116 & 0.0066 & 1.78 & 0.0767 \\ 
%   \hline
%		&&&&\small$df=323$ \\
%\end{tabular}\caption{ANOVA summary for testing whether the average on-base percentage differs across player positions.}
%\label{regressionSummaryTableForOBPAgainstPosition}
%\end{table}
%
%The sample means for each group can be identified using the \emph{Estimate} column. The baseline group -- players who are catchers -- are represented by the \emph{(Intercept)} row. That is, the sample mean for the on-base percentage of catchers was 0.3226. The other group means are identified using their own estimate as a correction to the baseline group. For instance, the average on-base percentage of designated hitters is equal to the baseline group plus the estimate in the designated hitter row:
%$$\bar{y}_{DH} = baseline + estimate_{positionDH} = 0.3226 + 0.0252 = 0.3478$$
%
%\begin{exercise}
%Compute the sample means of the infield and outfield groups. Verify your solutions using Table~\ref{mlbHRPerABSummaryTable} on page~\pageref{mlbHRPerABSummaryTable}.
%\end{exercise}
%
%\begin{caution}
%{Do not rely on the SEs or p-values in a regression summary for an ANOVA analysis}
%The calculated estimates for each group (besides the baseline group) rely on the sum of two estimates. Both of these estimates have uncertainty associated with them (each have a standard error), which makes it more difficult to ascertain the standard error of a particular group's estimate.
%\end{caution}

