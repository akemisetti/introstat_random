
% BELOW ARE PIECES DELETED FROM THE ANOVA SECTION

\subsection{The $F$ test for the ANOVA framework}

%%% Sum of squares footnote start
\newcommand{\sumOfSquaresExplanationForFootnote}{We use the term \term{sum of squares total ($SST$)} to describe the sum of the squared differences of the outcomes and the overall mean, $\bar{x}$:
\begin{align*}
SST = \sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^2
\end{align*}
where $n$ is the total number of cases. For example, $n=327$ in baseball example. Notice that $SST$ is an unstandardized measure of the total variability in the outcome, and the standardized form (the variance) is equal to $\frac{1}{n-1}SST$. % (this standardized total is sometimes labeled $MST$ for \term{mean square total}).

The total sum of squares can be broken down into two pieces: the \term{sum of squares between groups ($SSG$)} and the \term{sum of squares of the model errors ($SSE$)}:
\begin{align*}
SST &= SSG + SSE  \\
	&= \text{unstandardized between-group variability} \\
	&\qquad+ \text{unstandardized within-group variability}
\end{align*}
We can compute $SSG$ as
\begin{align*}
SSG = \sum_{j=1}^{k} \left(\bar{x}_{j} - \bar{x}\right)^2
\end{align*}
where $\bar{x}$ is again the sample mean of all outcomes and $k$ represents the number of groups. We compute $SSE$ as $SST - SSG$.

Standardized forms of $SSG$ and $SSE$ will be used to represent the between-group and within-group variability expressions when we compute $F$. The standardized form of $SSG$, called \term{means square between the groups (MSG)} is equal to the ratio of $SSG$ and the degrees of freedom associated with $SSG$:
\begin{align*}
MSG = \frac{SSG}{df_{G}} = \frac{SSG}{k-1}
\end{align*}
Similarly, the standardized form of $SSE$ is equal to $SSE$ divided by its corresponding degrees of freedom:
\begin{align*}
MSE = \frac{SSE}{df_{E}} = \frac{SSe}{n-k-1}
\end{align*}
Finally, we can compute $F$ as $MSG/MSE$.}
%%% Sum of squares footnote end



There is one important remaining question: A benchmark is also necessary to 

The variance of the group means is defined as the \term{between-group variability}, and the variance of the residuals (errors) from the fitted model is called the \term{within-group variability}. If the null hypothesis is true, the within-group and between group variability will be about equal. That is, the following fraction, which we will use as a test statistic, will be roughly 1 if the null hypothesis is true:
\begin{align*}
F &= \frac{\text{between-group variability}}{\text{within-group variability}}
\end{align*}
%IA large between-group variability corresponds to the  relative  -- that is, the group means are quite different -- 
The computation of the between-group and within-group variabilities rely on a technique referred to as \term{sum of the squares}, and each variability has an associated degrees of freedom associated with it:
\begin{align*}
df_{G} &= k-1 &&\text{degrees of freedom for the between-group variability} \\
df_{E} &= n-k-1 &&\text{degrees of freedom for the within-group variability}
\end{align*}
When the the groups actually have the same mean, the test statistic $F$ will follow an \term{F distribution}, which has two parameter values: $df_{G}$ and $df_{E}$.

When the null hypothesis is true, $F$ will be approximately 1. When the alternative is true, we should expect to more differences in the group means, which would correspond to more between-group variability and larger values of $F$. Smaller values of $F$ would not provide evidence of a difference.

\begin{exercise}
Reexamine the A, B, and C groups. The group means are not obviously different. Would you expect the $F$ statistic to be close to one or not? Answer in the footnote\footnote{We probably would not have strong evidence to reject the null hypothesis, so the test statistic $F$ would probably be close to one.}.
\end{exercise}

\begin{example}{The $F$ statistic to test if the means of groups A, B, and C are different are different was $F=1.4347$ with 2 and 67 degrees of freedom. See Table~\ref{toyANOVAabcOutput} on page~\pageref{toyANOVAabcOutput} for the ANOVA (i.e. regression) summary, where the outcome variable was labeled \var{group}. The p-value for the test is 0.2454. What does this mean?}
A p-value larger than 0.05 indicates that we have not found strong evidence to reject the null hypothesis. That is, we do not have sufficient evidence to say the groups actually have a different mean.
\end{example}
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
group & 2 & 5.60 & 2.80 & 1.43 & 0.2454 \\ 
  Residuals & 67 & 130.76 & 1.95 &  &  \\ 
   \hline
\end{tabular}
\caption{ANOVA summary to test the null hypothesis that $\mu_A = \mu_B = \mu_C$ for the corresponding groups shown in Figure~\ref{toyANOVA}.}
\label{toyANOVAabcOutput}
\end{table}

The ANOVA summary in Table~\ref{toyANOVAabcOutput} has a similar structure of a regression summary. We can identify the test statistic in the ``F value'' column, and the p-value is again in the last column. The other columns provide additional details of how the $F$ statistic was computed:
\begin{align*}
F = \frac{SS_{group} / df_{group}}{SS_{Res} / df_{Res}} =  \frac{5.60 / 2}{130.76 / 67} = \frac{2.80}{1.95} = 1.43
\end{align*}
Generally we use a computer is compute $F$ and a p-value. Details of the calculation of $F$ are provided in the footnote\footnote{\sumOfSquaresExplanationForFootnote}.

\begin{exercise}
Table~\ref{toyANOVAdefOutput} shows an ANOVA summary for groups D, E, and F from Figure~\ref{toyANOVA}. Identify the $F$ statistic, the associated degrees of freedom, and the p-value. Answer in the footnote\footnote{$F=12.91$ on 2 and 67 degrees of freedom. The p-value is about zero.}.
\end{exercise}
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
 & Df & Sum Sq & Mean Sq & F value & Pr($>$F) \\ 
  \hline
group & 2 & 5.60 & 2.80 & 12.91 & 0.0000 \\ 
  Residuals & 67 & 14.53 & 0.22 &  &  \\ 
   \hline
\end{tabular}
\caption{ANOVA summary to test the null hypothesis that $\mu_D = \mu_E = \mu_F$ for the corresponding groups shown in Figure~\ref{toyANOVA}.}
\label{toyANOVAdefOutput}
\end{table}

\Summary{Also include a regression summary table to show how we obtain point estimates for each group (including the \term{baseline} group), and how we can get obtain an $F$ statistic for the overall model fit.}

\subsection{Comparing home runs per at bat across player positions}

\Summary{Pose research question. Have students identify what the degrees of freedom should be for the $F$ statistic.}

\subsection{Multiple comparisons and controlling the Type 1 Error rate}
\label{multipleComparisonsAndControllingTheType1ErrorRate}

\Summary{Initially prompt students to think about looking at many many comparisons (e.g. 9 groups and 36 comparisons). Indicate that when we are comparing many groups to identify differences, we make it harder to reject the null hypothesis. Note that it is possible to reject the null that all the means are the same but not be able to identify which pairs are actually different.}

\subsection{Using ANOVA for multiple regression}



%\Summary{Connect salary data to the }


%%%%%
\section{Logistic regression for a binomial outcome}

\subsection{Evaluating the 2010 House election using 2008 results}

\subsection{Modeling the probability of an event}

\subsection{Fitting a model to}




