\chapter{Probability (special topic)}
\label{probability}

%{\begin{floatingfigure}[r]{1.85in} \includegraphics[width=1.6in]{02/figures/dice/dice} \end{floatingfigure}}
Probability forms a foundation for statistics. You may already be familiar with many aspects of probability, however, formalization of the concepts is new for most. This chapter aims to introduce probability on familiar terms using processes most people have seen before.

\vspace{5mm}

\section{Defining probability (special topic)}
\label{basicsOfProbability}

\begin{example}{A ``die'', the singular of dice, is a cube with six faces numbered \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, and \resp{6}. What is the chance of getting \resp{1} when rolling a die?}\label{probOf1}
If the die is fair, then the chance of a \resp{1} is as good as the chance of any other number. Since there are six outcomes, the chance must be 1-in-6 or, equivalently, $1/6$.
\end{example}

\begin{example}{What is the chance of getting a \resp{1} or \resp{2} in the next roll?}\label{probOf1Or2}
\resp{1} and \resp{2} constitute two of the six equally likely possible outcomes, so the chance of getting one of these two outcomes must be $2/6 = 1/3$.
\end{example}

\begin{example}{What is the chance of getting either \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6} on the next roll?}\label{probOf123456}
100\%. The die must be one of these numbers.
\end{example}

\begin{example}{What is the chance of not rolling a \resp{2}?}\label{probNot2}
Since the chance of rolling a \resp{2} is $1/6$ or $16.\bar{6}\%$, the chance of not getting a \resp{2} must be $100\% - 16.\bar{6}\%=83.\bar{3}\%$ or $5/6$.

Alternatively, we could have noticed that not rolling a \resp{2} is the same as getting a \resp{1}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}, which makes up five of the six equally likely outcomes and has probability $5/6$.
\end{example}

\begin{example}{Consider rolling two dice. If $1/6^{th}$ of the time the first die is \resp{1} and $1/6^{th}$ of those times the second die is a \resp{1}, what is the chance of getting two \resp{1}s?}\label{probOf2Ones}
If $16.\bar{6}$\% of the time the first die is a \resp{1} and $1/6^{th}$ of \emph{those} times the second die is also a \resp{1}, then the chance both dice are \resp{1} is $(1/6)*(1/6)$ or $1/36$.
\end{example}

\subsection{Probability}

We use probability to build tools to describe and understand apparent randomness. We often frame probability in terms of a \term{random process} giving rise to an \term{outcome}.
\begin{center}
\begin{tabular}{lll}
Roll a die &$\rightarrow$ & \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6} \\
Flip a coin &$\rightarrow$ & \resp{H} or \resp{T} \\
\end{tabular}
\end{center}
Rolling a die or flipping a coin is a seemingly random process and each gives rise to an outcome. 

\begin{termBox}{\tBoxTitle{Probability}
The \term{probability} of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times.}
\end{termBox}

Probability is defined as a proportion, and it must always be between 0 and 1 (inclusively). It may also be displayed as a percentage between 0\% and 100\%.

Probability can be illustrated by rolling a die many times. Let $\hat{p}_n$ be the proportion of outcomes that are \resp{1} after the first $n$ rolls. As the number of rolls increases, $\hat{p}_n$ will converge to the probability of rolling \resp{1}, $p = 1/6$. Figure~\ref{dieProp} shows this convergence for 100,000 die rolls. The tendency of $\hat{p}_n$ to stabilize around $p$ is described by the \term{Law of Large Numbers}. 
\begin{figure}[bt]
\centering
\includegraphics[width=0.85\textwidth]{02/figures/dieProp/dieProp}
\caption{The fraction of die rolls that are 1 at each stage in a simulation. The proportion tends to get closer to the probability $1/6 \approx 0.167$ as the sample size gets large.}
\label{dieProp}
\end{figure}

\begin{termBox}{\tBoxTitle{Law of Large Numbers}
As more observations are collected, the proportion $\hat{p}_n$ of occurrences with a particular outcome converges to the probability $p$ of that outcome.}
\end{termBox}

Occasionally the proportion will veer off from the probability and appear to defy the Law of Large Numbers, as $\hat{p}_n$ does many times in Figure~\ref{dieProp}. However, these deviations become smaller as the sample size becomes larger.

Above we write $p$ as the probability of rolling a \resp{1}. We can also write this probability as
\begin{eqnarray*}
P(\text{rolling a \resp{1}})
\end{eqnarray*}
\marginpar[\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$]{\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$}As we become more comfortable with this notation, we will abbreviate it further. For instance, if it is clear the process is ``rolling a die'', we could abbreviate $P($rolling a \resp{1}$)$ as $P($\resp{1}$)$. 

\begin{exercise} \label{randomProcessExercise}
Random processes include rolling a die and flipping a coin. (a) Think of another random process. (b) Describe all the possible outcomes of that process. For instance, rolling a die is a random process with potential outcomes \resp{1}, \resp{2}, ..., \resp{6}. Several examples are in the footnote\footnote{(i) Whether someone gets sick in the next month or not is an apparently random process with outcomes \resp{sick} and \resp{not}. (ii) We can \emph{generate} a random process by randomly picking a person and measuring that person's height. The outcome of this process will be a positive number. (iii) Whether the stock market goes up or down next week is a seemingly random process with possible outcomes \resp{up}, \resp{down}, and \resp{noChange}. Alternatively, we could have used the percent change in the stock market as a numerical outcome. (iv) Whether your roommate cleans her dishes tonight probably seems like a random process with possible outcomes \resp{cleansDishes} and \resp{leavesDishes}.}.
\end{exercise}

What we think of as random processes are not necessarily random, but they may just be too difficult to understand exactly. Example (iv) in Exercise~\exer{randomProcessExercise} suggests whether your roommate does her dishes tonight is random. However, even if your roommate's behavior is not truly random, modeling her behavior as a random process can still be useful. 

\begin{tipBox}{\tipBoxTitle{Modeling a process as random}
It can be helpful to model a process as random even if it is not truly random.}
\end{tipBox}

\subsection{Disjoint or mutually exclusive outcomes}

Two outcomes are called \term{disjoint} or \term{mutually exclusive} if they cannot both happen. For instance, if we roll a die, the outcomes \resp{1} and \resp{2} are disjoint since they cannot both occur. On the other hand, the outcomes \resp{1} and ``rolling an odd number'' are not disjoint since both occur if the die is a \resp{1}. The terms \emph{disjoint} and \emph{mutually exclusive} are equivalent and interchangeable.

Calculating the probability of disjoint outcomes is easy. When rolling a die, the outcomes \resp{1} and \resp{2} are disjoint, and we compute the probability that one of these outcomes will occur by adding their separate probabilities:
\begin{eqnarray*}
P(\text{\resp{1} or \resp{2}}) = P(\text{\resp{1}})+P(\text{\resp{2}}) = 1/6 + 1/6 = 1/3
\end{eqnarray*}
What about  the probability of rolling a \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}? Here again, all of the outcomes are disjoint so we add the probabilities:
\begin{eqnarray*}
&&P(\text{\resp{1} or \resp{2} or \resp{3} or \resp{4} or \resp{5} or \resp{6}}) \\
	&&\quad= P(\text{\resp{1}})+P(\text{\resp{2}})+P(\text{\resp{3}})+P(\text{\resp{4}})+P(\text{\resp{5}})+P(\text{\resp{6}}) \\
	&&\quad= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
\end{eqnarray*}
The \term{Addition Rule} guarantees the accuracy of this approach when the outcomes are disjoint. 

\begin{termBox}{\tBoxTitle{Addition Rule of disjoint outcomes} If $A_1$ and $A_2$ represent two disjoint outcomes, then the probability that one of them occurs is given by
\begin{eqnarray*}
P(A_1\text{ or } A_2) = P(A_1) + P(A_2)
\end{eqnarray*}
If there are many disjoint outcomes $A_1$, ..., $A_k$, then the probability that one of these outcomes will occur is
\begin{eqnarray}
P(A_1) + P(A_2) + \cdots + P(A_k)
\end{eqnarray}
}
\end{termBox}

\begin{exercise}
We are interested in the probability of rolling a \resp{1}, \resp{4}, or \resp{5}. (a) Explain why the outcomes \resp{1}, \resp{4}, and \resp{5} are disjoint. (b) Apply the Addition Rule for disjoint outcomes to determine $P($\resp{1} or \resp{4} or \resp{5}$)$.
\end{exercise}

\begin{exercise}
In the \data{cars} data set in Chapter~1% ZZQ\ref{introductionToData}
, the \var{type} variable described the size of the vehicle: \resp{small} (21 cars), \resp{midsize} (22 cars), or \resp{large} (11 cars). (a) Are the outcomes \resp{small}, \resp{midsize}, and \resp{large} disjoint? Answer in the footnote\footnote{Yes. Each car is categorized in only one level of \var{type}.}. (b) Determine the proportion of \resp{midsize} and \resp{large} cars separately. (c) Use the Addition Rule for disjoint outcomes to compute the probability a randomly selected car from this sample is either \resp{midsize} or \resp{large}.
\end{exercise}

Statisticians rarely work with individual outcomes and instead consider \indexthis{\emph{sets}}{sets} or \indexthis{\emph{collections}}{collections} of outcomes. Let $A$ represent the event where a die roll results in \resp{1} or \resp{2} and $B$ represent the event that the die roll is a \resp{4} or a \resp{6}. We write $A$ as the set of outcomes $\{$\resp{1}, \resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$. These sets are commonly called \term{events}. Because $A$ and $B$ have no elements in common, they are disjoint events. $A$ and $B$ are represented in Figure~\ref{disjointSets}.
\begin{figure}[hhh]
\centering
\includegraphics[height=0.7in]{02/figures/disjointSets/disjointSets}
\caption{Three events, $A$, $B$, and $D$, consist of outcomes from rolling a die. $A$ and $B$ are disjoint since they do not have any outcomes in common.}
\label{disjointSets}
\end{figure}

\begin{exercise}
Events $A$ and $B$ in Figure~\ref{disjointSets} are disjoint. What other pair of events in the figure are disjoint? Answer in the footnote\footnote{Sets $B$ and $D$ since they have no outcomes in common.}.
\end{exercise}

The Addition Rule applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events $A$ or $B$ occurs is the sum of the separate probabilities:
\begin{eqnarray*}
P(A\text{ or }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3
\end{eqnarray*}

\begin{exercise}
(a) Verify the probability of event $A$, $P(A)$, is $1/3$ using the Addition Rule for disjoint events. (b) Do the same for $B$.
\end{exercise}

\begin{exercise} \label{exerExaminingDisjointSetsABD}
(a) Using Figure~\ref{disjointSets} as a reference, what outcomes are represented by the event $D$? (b) Are events $B$ and $D$ disjoint? (c) Are events $A$ and $D$ disjoint? Answer to part (c) is in the footnote\footnote{The events $A$ and $D$ share an outcome in common, \resp{2}, and so are not disjoint.}.
\end{exercise}

\begin{exercise}
In Exercise~\exer{exerExaminingDisjointSetsABD}, you confirmed $B$ and $D$ from Figure~\ref{disjointSets} are disjoint. Compute the probability that either event $B$ or event $D$ occurs.
\end{exercise}

\subsection{Probabilities when events are not disjoint}

Let's consider calculations for two events that are not disjoint in the context of a \indexthis{regular deck of 52 cards}{deck of cards}, represented in Table~\ref{deckOfCards}. If you are unfamiliar with the cards in a regular deck, please see the footnote\footnote{The 52 cards are split into four \term{suits}: $\clubsuit$ (club), $\diamondsuit$ (diamond), $\heartsuit$ (heart), $\spadesuit$ (spade). Each suit has its 13 cards labeled: \resp{2}, \resp{3}, ..., \resp{10}, \resp{J} (jack), \resp{Q} (queen), \resp{K} (king), and \resp{A} (ace). Thus, each card is a unique combination of a suit and a label, e.g. \resp{4$\heartsuit$}. The 12 cards represented by the jacks, queens, and kings are called \termsub{\resp{face cards}}{face card}. The cards that are $\diamondsuit$ or $\heartsuit$ are typically colored \resp{red} while the other two suits are typically colored \resp{black}.}. 
\begin{table}
\centering
\begin{tabular}{lll lll lll lll l}
\resp{2$\clubsuit$} & \resp{3$\clubsuit$} & \resp{4$\clubsuit$} & \resp{5$\clubsuit$} & \resp{6$\clubsuit$} & \resp{7$\clubsuit$} & \resp{8$\clubsuit$} & \resp{9$\clubsuit$} & \resp{10$\clubsuit$} & \resp{J$\clubsuit$} & \resp{Q$\clubsuit$} & \resp{K$\clubsuit$} & \resp{A$\clubsuit$}  \\
\color{red} \resp{2$\diamondsuit$} & \color{red}\resp{3$\diamondsuit$} & \color{red}\resp{4$\diamondsuit$} & \color{red}\resp{5$\diamondsuit$} & \color{red}\resp{6$\diamondsuit$} & \color{red}\resp{7$\diamondsuit$} & \color{red}\resp{8$\diamondsuit$} & \color{red}\resp{9$\diamondsuit$} & \color{red}\resp{10$\diamondsuit$} & \color{red}\resp{J$\diamondsuit$} & \color{red}\resp{Q$\diamondsuit$} & \color{red}\resp{K$\diamondsuit$} & \color{red}\resp{A$\diamondsuit$} \\
\color{red}\resp{2$\heartsuit$} & \color{red}\resp{3$\heartsuit$} & \color{red}\resp{4$\heartsuit$} & \color{red}\resp{5$\heartsuit$} & \color{red}\resp{6$\heartsuit$} & \color{red}\resp{7$\heartsuit$} & \color{red}\resp{8$\heartsuit$} & \color{red}\resp{9$\heartsuit$} & \color{red}\resp{10$\heartsuit$} & \color{red}\resp{J$\heartsuit$} & \color{red}\resp{Q$\heartsuit$} & \color{red}\resp{K$\heartsuit$} & \color{red}\resp{A$\heartsuit$} \\
\resp{2$\spadesuit$} & \resp{3$\spadesuit$} & \resp{4$\spadesuit$} & \resp{5$\spadesuit$} & \resp{6$\spadesuit$} & \resp{7$\spadesuit$} & \resp{8$\spadesuit$} & \resp{9$\spadesuit$} & \resp{10$\spadesuit$} & \resp{J$\spadesuit$} & \resp{Q$\spadesuit$} & \resp{K$\spadesuit$} & \resp{A$\spadesuit$}
\end{tabular}
\caption{Representations of the 52 unique cards in a deck.}
\label{deckOfCards}
\end{table}

\begin{exercise}
(a) What is the probability a randomly selected card is a diamond? (b) What is the probability a randomly selected card is a face card?
\end{exercise}

\term{Venn diagrams} are useful when outcomes can be categorized as ``in'' or ``out'' for two or three variables, attributes, or random processes. The Venn diagram in Figure~\ref{venn} uses a circle to represent diamonds and another to represent face cards. If a card is both a diamond and a face card, it falls into the intersection of the circles. If it is a diamond but not a face card, it will be in part of the left circle that is not in the right circle (and so on). The total number of cards that are diamonds is given by the total number of cards in the diamonds circle: $10+3=13$. The probabilities are shown in \color{gray}gray\color{black}. 
\begin{figure}
\centering
\includegraphics[height=1.4in]{02/figures/venn/venn}
\caption{A Venn diagram for diamonds and face cards.}
\label{venn}
\end{figure}

\begin{exercise}
Using the Venn diagram, verify $P($face card$) = 12/52=3/13$.
\end{exercise}

Let $A$ represent the event that a randomly selected card is a diamond and $B$ represent the event it is a face card. How do we compute $P(A$ or $B)$? Events $A$ and $B$ are not disjoint -- the cards $J\diamondsuit$, $Q\diamondsuit$, and $K\diamondsuit$ fall into both categories -- so we cannot use the Addition Rule for disjoint events. Instead we use the Venn diagram. We start by adding the probabilities of the two events:
\begin{eqnarray*}
P(A) + P(B) = P(\diamondsuit) + P(\text{face card}) = 12/52 + 13/52
\label{overCountFaceDiamond}
\end{eqnarray*}
However, the three cards that are in both events were counted twice, once in each probability. We must correct this double counting:
\begin{eqnarray}
P(A\text{ or } B) &=&P(\text{face card or }\diamondsuit)  \notag \\
 &=& P(\text{face card}) + P(\diamondsuit) - P(\text{face card \& }\diamondsuit) \label{diamondFace} \\
 &=& 12/52 + 13/52 - 3/52 \notag \\
 &=& 22/52 = 11/26 \notag
\end{eqnarray}
Equation~(\ref{diamondFace}) is an example of the \term{General Addition Rule}. 

\begin{termBox}{\tBoxTitle{General Addition Rule} If $A$ and $B$ are any two events, disjoint or not, then the probability that at least one of them will occur is
\begin{eqnarray}
P(A) + P(B) - P(A\text{ \& }B)
\label{generalAdditionRule}
\end{eqnarray}
where $P(A$ \& $B)$ is the probability that both events occur.}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{``or'' is inclusive}
When we write ``or'' in statistics, we mean ``and/or'' unless we explicitly state otherwise. Thus, $A$ or $B$ occurs means $A$, $B$, or both $A$ and $B$ occur.}
\end{tipBox}

\begin{exercise}
(a) Describe why, if $A$ and $B$ are disjoint, then $P(A$ \& $B) = 0$. (b) Using part (a), verify that the General Addition Rule simplifies to the Addition Rule for disjoint events if $A$ and $B$ are disjoint. Answers in the footnote\footnote{(a) If $A$ and $B$ are disjoint, $A$ and $B$ can never occur simultaneously. (b) If $A$ and $B$ are disjoint, then the last term of Equation~(\ref{generalAdditionRule}) is 0 (see part (a)) and we are left with the Addition Rule for disjoint events.}.
\end{exercise}

\begin{exercise}\label{carsTypeCapacityVennExer}
In the \data{cars} data set with 54 vehicles, 22 were \resp{midsize} cars, 16 had a capacity of \resp{6} people, and 5 \resp{midsize} cars had a capacity of \resp{6} people. Create a Venn diagram for this setup. Answer in the footnote\footnote{\vspace{-3mm}

\noindent\includegraphics[height=20mm]{02/figures/carsTypeCapacityVenn/carsTypeCapacityVenn}}.
\end{exercise}

\begin{exercise}
(a) Use your Venn diagram from Exercise~\exer{carsTypeCapacityVennExer} to determine the probability a random car from the \data{cars} data set is both a \resp{midsize} vehicle and has a capacity of \resp{6}. (b) What is the probability the car is a \resp{midsize} car or has a \resp{6} person capacity?
\end{exercise}

\subsection{Probability distributions}

The grocery receipt for a college student is shown in Table~\ref{expendituresCollegeStudent1}. Does anything seem odd about the total? The individual costs only add up to \$23.20 while the total is written as \$37.90. Where did the additional \$14.70 come from?
\begin{table}[ht]
\centering
\begin{tabular}{l r}
  \hline
  Item & Cost \\
  \hline
  Spaghetti & \$6.50 \\
  Carrots & \$7.20 \\
  Apples & \$3.10 \\
  Milk & \$6.40 \\
  Tax & \$0.00 \\
  \hline
  Total & \$37.90 \\
  \hline
\end{tabular}
\caption{Grocery receipt with a total greater than the sum of the costs.}
\label{expendituresCollegeStudent1}
\end{table}

Table~\ref{expendituresCollegeStudent2} shows another month of expenditures with a new problem. While the sum of the expenditures match up, the amount spent on milk is a negative.
\begin{table}[ht]
\centering
\begin{tabular}{l r}
  \hline
  Item & Cost \\
  \hline
  Spaghetti & \$6.50 \\
  Carrots & \$7.20 \\
  Apples & \$5.10 \\
  Milk & \textbf{-}\$4.40 \\
  Chocolate & \$1.19 \\
  Tax & \$0.11 \\
  \hline
  Total & \$15.70 \\
  \hline
\end{tabular}
\caption{On this receipt, buying milk saves the customer money (!).}
\label{expendituresCollegeStudent2}
\end{table}


A \term{probability distribution} is a table of all disjoint outcomes and their associated probabilities. It is like a grocery bill, except that instead of foods there are outcomes, and instead of costs for each food item there are probabilities for each outcome. Table~\ref{diceProb} shows the probability distribution for the sum of two dice. 

\begin{table}[h] \small
\centering
\begin{tabular}{l ccc ccc ccc cc}
  \hline
Dice sum & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12  \\
Probability & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\vspace{0.1cm} \\
   \hline
\end{tabular}
\caption{Probability distribution for the sum of two dice.}
\label{diceProb}
\end{table}

Probability distributions share the same structure as grocery receipts. However, probability distributions impose one special rule: the total must be 1. 

\begin{termBox}{\tBoxTitle{Rules for probability distributions}
A probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules: \vspace{-2mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item The outcomes listed must be disjoint.
\item Each probability must be between 0 and 1.
\item The probabilities must total 1. \vspace{1mm}
\end{enumerate}}
\end{termBox}

\vspace{-10mm}

\begin{exercise}\label{usHouseholdIncomeDistsExercise}
Table~\ref{usHouseholdIncomeDists} suggests three distributions for household income in the United States. Only one is correct. Which one must it be? What is wrong with the other two? Answer in the footnote\footnote{The probabilities of (a) do not sum to 1. The second probability in (b) is negative. This leaves (c), which sure enough satisfies the requirements of a distribution. One of the three was said to be the actual distribution of US household incomes, so it must be (c).}.
\end{exercise}
\begin{table}
\centering
\begin{tabular}{l | rr rr}
  \hline
Income range (\$1000s) & 0-25    & 25-50    & 50-100     & 100+    \\
  \hline
(a)				 & 0.18 & 0.39 & 0.33 & 0.16 \\
(b)				 & 0.38 & -0.27 & 0.52 & 0.37 \\
(c)				 & 0.28 & 0.27 & 0.29 & 0.16 \\
  \hline
\end{tabular}
\caption{Proposed distributions of US household incomes (Exercise~\exer{usHouseholdIncomeDistsExercise}).}
\label{usHouseholdIncomeDists}
\end{table}

Chapter~\ref{introductionToData} emphasized the importance of plotting data to provide quick summaries. Probability distributions can also be summarized in a bar plot. For instance, the distribution of US household incomes is shown in Figure~\ref{usHouseholdIncomeDistBar} as a bar plot\footnote{It is also possible to construct a distribution plot when income is not artificially binned into four groups. \emph{Continuous} distributions are considered in Section~\ref{contDist}.}. The probability distribution for the sum of two dice is shown in Table~\ref{diceProb} and plotted in Figure~\ref{diceSumDist}.

\begin{figure}
\centering
\includegraphics[height=2in]{02/figures/usHouseholdIncomeDistBar/usHouseholdIncomeDistBar}
\caption{The probability distribution of US household income.}
\label{usHouseholdIncomeDistBar}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=2in]{02/figures/diceSumDist/diceSumDist}
\caption{The probability distribution of the sum of two dice.}
\label{diceSumDist}
\end{figure}

In these bar plots, the bar heights represent the outcome probabilities. If the outcomes are numerical and discrete, it is usually (visually) convenient to place the bars at their associated locations on the axis, as in the case of the sum of two dice. Another example of plotting the bars at their respective locations is shown in Figure~\ref{bookCostDist} on page~\pageref{bookCostDist}.

\subsection{Complement of an event}

%Every simple process has an outcome. 
Rolling a die produces a value in the set $\{$\resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$. This set of all possible outcomes is called the \term{sample space} ($S$) for rolling a die. We often use the sample space to examine the scenario where an event does not occur.

Let $D=\{$\resp{2}, \resp{3}$\}$ represent the event we roll a die and get \resp{2} or \resp{3}. Then the \term{complement}\marginpar[\raggedright$A^c$\\\footnotesize Complement\\of outcome $A$]{\raggedright$A^c$\\\footnotesize Complement\\of outcome $A$} of $D$ represents all outcomes in our sample space that are not in $D$, which is denoted by $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. That is, $D^c$ is the set of all possible outcomes not already included in $D$. Figure~\ref{complementOfD} shows the relationship between $D$, $D^c$, and the sample space $S$. 
\begin{figure}[hht]
\centering
\includegraphics[height=1in]{02/figures/complementOfD/complementOfD}
\caption{Event $D=\{$\resp{2}, \resp{3}$\}$ and its complement, $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. $S$ represents the sample space, which is the set of all possible events.}
\label{complementOfD}
\end{figure}

\begin{exercise}
(a) Compute $P(D^c) = P($rolling a \resp{1}, \resp{4}, \resp{5}, or \resp{6}$)$. (b) What is $P(D) + P(D^c)$?
\end{exercise}

\begin{exercise}
Events $A=\{$\resp{1}, \resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$ are shown in Figure~\ref{disjointSets} on page~\pageref{disjointSets}. (a) Write out what $A^c$ and $B^c$ represent. (b) Compute $P(A^c)$ and $P(B^c)$. (c) Compute $P(A)+P(A^c)$ and $P(B)+P(B^c)$.
\end{exercise}

A complement of an event $A$ is constructed to have two very important properties: (i) every possible outcome not in $A$ is in $A^c$, and (ii) $A$ and $A^c$ are disjoint. Property (i) implies
\begin{eqnarray}
P(A\text{ or }A^c) = 1
\label{complementSumTo1}
\end{eqnarray}
That is, if the outcome is not in $A$, it must be represented in $A^c$. We use the Addition Rule for disjoint events to apply Property (ii):
\begin{eqnarray}
P(A\text{ or }A^c) = P(A) + P(A^c)
\label{complementDisjointEquation}
\end{eqnarray}
Combining Equations~(\ref{complementSumTo1}) and~(\ref{complementDisjointEquation}) yields a very useful relationship between the probability of an event and its complement.

\begin{termBox}{\tBoxTitle{Complement}
The complement of event $A$ is denoted $A^c$, and $A^c$ represents all outcomes not in $A$. $A$ and $A^c$ are mathematically related: \vspace{-2mm}
\begin{eqnarray}\label{complement}
P(A) + P(A^c) = 1, \quad\text{i.e.}\quad P(A) = 1-P(A^c)
\end{eqnarray}\vspace{-6.5mm}}
\end{termBox}

In simple examples, computing $A$ or $A^c$ is feasible in a few steps. However, using the complement can save a lot of time as problems grow in complexity.

\begin{exercise}
Let $A$ represent the event where we roll two dice and their total is less than \resp{12}. (a) What does the event $A^c$ represent? Answer in the footnote\footnote{The complement of $A$: when the total is equal to \resp{12}.}. (b) Determine $P(A^c)$ from Table~\ref{diceProb} on page~\pageref{diceProb}. (c) Determine $P(A)$. Answer in the footnote\footnote{Use the probability of the complement from part (b), $P(A^c) = 1/36$, and Equation~(\ref{complement}): $P($less than \resp{12}$) = 1 - P($\resp{12}$) = 1 - 1/36 = 35/36$.}.
\end{exercise}

\begin{exercise} Consider again the probabilities from Table~\ref{diceProb} and rolling two dice. Find the following probabilities: (a) The sum of the dice is \emph{not} \resp{6}. Hint in the footnote\footnote{First find $P($\resp{6}$)$.}. (b) The sum is at least \resp{4}. That is, determine the probability of the event $B=\{$\resp{4}, \resp{5}, ..., \resp{12}$\}$. Answer in the footnote\footnote{We first find  the complement, which requires much less effort: $P($\resp{2} or \resp{3}$)=1/36+2/36=1/12$. Then we find $P(B) = 1-P(B^c) = 1-1/12 = 11/12$.}. (c) The sum is no more than \resp{10}. That is, determine the probability of the event $D=\{$\resp{2}, \resp{3}, ..., \resp{10}$\}$.
\end{exercise}


\subsection{Independence}
\label{probabilityIndependence}

Just as variables and observations can be independent, random processes can be independent, too. Two processes are \term{independent} if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes -- knowing the coin was \resp{H} does not help determine what the die will be. On the other hand, stock prices usually move up or down together, so they are not independent.

Example~\exam{probOf2Ones} provides a basic example of two independent processes: rolling two dice. We want to determine the probability that both will be \resp{1}. Suppose one of the dice is red and the other white. If the outcome of the red die is a \resp{1}, it provides no information about the outcome of the white die. Example~\exam{probOf2Ones}'s argument (page~\pageref{probOf2Ones}) is as follows: $1/6^{th}$ of the time the red die is \resp{1}, and $1/6^{th}$ of \emph{those} times the white die will also be \resp{1}. This is illustrated in Figure~\ref{indepForRollingTwo1s}. Because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to get the final answer: $(1/6)*(1/6)=1/36$. This can be generalized to many independent processes. 
\begin{figure}[hht]
\centering
\includegraphics[height=1.8in]{02/figures/indepForRollingTwo1s/indepForRollingTwo1s}
\caption{$1/6^{th}$ of the time, the first roll is a \resp{1}. Then $1/6^{th}$ of \emph{those} times, the second roll will also be a \resp{1}.}
\label{indepForRollingTwo1s}
\end{figure}

\begin{example}{What if there was also a blue die independent of the other two? What is the probability of rolling the three dice and getting all \resp{1}s?}\label{threeDice}
The same logic applies from Example~\exam{probOf2Ones}. If $1/36^{th}$ of the time the white and red dice are both \resp{1}, then $1/6^{th}$ of \emph{those} times the blue die will also be \resp{1}, so multiply:
\begin{eqnarray*}
&&P(white=\text{ \resp{1} and } red=\text{ \resp{1} and } blue=\text{ \resp{1}}) \\
&&\quad= P(white=\text{ \resp{1}})*P(red=\text{ \resp{1}})*P(blue=\text{ \resp{1}}) \\
&&\quad = (1/6)*(1/6)*(1/6) = (1/36)*(1/6) = 1/216
\end{eqnarray*}
\end{example}

Examples~\exam{probOf2Ones} and~\exam{threeDice} illustrate what is called the Product Rule for independent processes. 

\begin{termBox}{\tBoxTitle{\term{Product Rule for independent processes}}
If $A$ and $B$ represent events from two different and independent processes, then the probability both $A$ and $B$ occur can be computed as the product of their separate probabilities: \vspace{-1.5mm}
\begin{eqnarray}\label{eqForIndependentEvents}
P(A \text{ and }B) = P(A) * P(B)
\end{eqnarray}
Similarly, if there are $k$ events $A_1$, ..., $A_k$ from $k$ independent processes, then the probability they all occur is\vspace{-1.5mm}
\begin{eqnarray*}
P(A_1) * P(A_2)* \cdots * P(A_k)
\end{eqnarray*}\vspace{-7mm}}
\end{termBox}

\begin{exercise} \label{ex2Handedness}
About 9\% of people are \resp{leftHanded}. Suppose 2 people are selected at random from the U.S. population. Because the sample of 2 is so small relative to the population, it is reasonable to assume these two people are independent. (a)~What is the probability that both are \resp{leftHanded}? (b)~What is the probability that both are \resp{rightHanded}? Answer to (a) and hint to (b) in the footnote\footnote{(a) The probability the first person is \resp{leftHanded} is $0.09$, which is the same for the second person. We apply the Product Rule for independent processes to determine the probability that both will be \resp{leftHanded}: $0.09*0.09 = 0.0081$.

(b) It is reasonable to assume the proportion of people who are ambidextrous (both right and left handed) is nearly 0, which results in $P($\resp{rightHanded}$)=1-0.09=0.91$.}.
\end{exercise}

\begin{exercise} \label{ex5Handedness}
Suppose 5 people are selected at random. \vspace{-1.5mm}
\begin{enumerate}
\item[(a)] What is the probability that all are \resp{rightHanded}? Answer in the footnote\footnote{Since each are independent, we apply the Product Rule for independent processes:
\begin{eqnarray*}
&&P(\text{all five are \resp{rightHanded}}) \\
&&\quad = P(\text{first = \resp{rH}, second = \resp{rH}, ..., fifth = \resp{rH}}) \\
&&\quad = P(\text{first = \resp{rH}})*P(\text{second = \resp{rH}})* \dots *P(\text{fifth = \resp{rH}}) \\
&&\quad = 0.91*0.91*0.91*0.91*0.91 = 0.624
\end{eqnarray*}\vspace{-4mm}}.
\item[(b)] What is the probability that all are \resp{leftHanded}?
\item[(c)] What is the probability that not all of the people are \resp{rightHanded}? Hint in the footnote\footnote{Use the complement, $P($all five are \resp{rightHanded}$)$, to answer this question.}.
\end{enumerate}
\end{exercise}

Suppose the variables \var{handedness} and \var{gender} are independent, i.e. knowing someone's \var{gender} provides no useful information about their \var{handedness} and vice-versa. We can compute whether a randomly selected person is \resp{rightHanded} and \resp{female}\footnote{The actual proportion of the U.S. population that is \resp{female} is about 50\%, and so we use 0.5 for the probability of sampling a woman. However, this probability is different in other countries.} using the Product Rule:
\begin{eqnarray*}
P(\text{\resp{rightHanded} and \resp{female}}) &=& P(\text{\resp{rightHanded}}) * P(\text{\resp{female}}) \\
&=& 0.91 * 0.50 = 0.455
\end{eqnarray*}


\begin{exercise}
Three people are selected at random. Answers to each part are in the footnote\footnote{This is the same as $P($a randomly selected person is \resp{male} and \resp{rightHanded}$)=0.455$. \par \hspace{3mm} (b) 0.207. (c) 0.045. (d) 0.0093.}. (a) What is the probability the first person is \resp{male} and \resp{rightHanded}? (b) What is the probability the first two people are \resp{male} and \resp{rightHanded}?. (c) What is the probability the third person is \resp{female} and \resp{leftHanded}? (d) What is the probability the first two people are \resp{male} and \resp{rightHanded} and the third person is \resp{female} and \resp{leftHanded}?
\end{exercise}

Sometimes we wonder if one outcome provides useful information about another outcome. The question we are asking is, are the occurrences of the two events independent? We say that two events $A$ and $B$ are independent if they satisfy Equation~\eqref{eqForIndependentEvents}.

\begin{example}{If we shuffle up a deck of cards and draw one, is the event that the card is a heart independent of the event that the card is an ace?}
The probability the card is a heart is $1/4$ and the probability it is an ace is $1/13$. The probability the card is the ace of hearts is $1/52$. We check whether Equation~\ref{eqForIndependentEvents} is satisfied:
\begin{align*}
P({\color{red}\heartsuit})*P(\text{ace}) &= \frac{1}{4}*\frac{1}{13} \\
					&= \frac{1}{52} \\
					&= P({\color{red}\heartsuit}\text{ \& ace})
\end{align*}
Because the equation holds, the event that the card is a heart and the event that the card is an ace are independent events.
\end{example}

%%%%%%%%%%%%%%%%%
\section{Continuous distributions (special topic)}
\label{contDist}

\begin{example}{Figure~\ref{fdicHistograms} shows a few different hollow histograms of the variable \var{height} for 3 million US adults from the mid-90's\footnote{This sample can be considered a simple random sample from the US population. It relies on the USDA Food Commodity Intake Database.}. How does changing the number of bins allow you to make different interpretations of the data?}\label{usHeights}
Adding more bins provides greater detail. This sample is extremely large, which is why much smaller bins still work well. Usually we do not use so many bins with smaller sample sizes since small counts per bin mean the bin heights are very volatile.
\end{example}
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{02/figures/fdicHistograms/fdicHistograms}
\caption{Four hollow histograms of US adults heights with varying bin widths.}
\label{fdicHistograms}
\end{figure}

\begin{example}{What proportion of the sample is between \resp{180} cm and \resp{185} cm tall (about 5'11" to 6'1")?}\label{contDistProb}
The probability that a randomly selected person is between \resp{180} cm and \resp{185} cm can be estimated by finding the proportion of people in the sample who are between these two heights. This means adding up the heights of the bins in this range and dividing by the sample size. For instance, this can be done with the two shaded bins in Figure~\ref{usHeightsHist180185}. The two bins in this region have counts of 195,307 and 156,239 people, resulting in the following estimate of the probability:
\begin{eqnarray*}
\frac{195307+156239}{\text{3,000,000}} = 0.1172
\end{eqnarray*}
This fraction is the same as the proportion of the histogram's area that falls in the range \resp{180} to \resp{185} cm. %The idea of quantifying a proportion or probability as an area under a curve is important to many areas of statistics.
\end{example}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{02/figures/usHeightsHist180185/usHeightsHist180185}
\caption{A histogram with bin sizes of 2.5 cm. The shaded region represents individuals with heights between \resp{180} and \resp{185} cm. }
\label{usHeightsHist180185}
\end{figure}

\subsection{From histograms to continuous distributions}

Examine the transition from a boxy hollow histogram in the top-left of Figure~\ref{fdicHistograms} to the much more smooth plot in the lower-right. In this last plot, the bins are so slim that the hollow histogram is starting to resemble a smooth curve. This suggests the population height as a \emph{continuous} numerical variable might best be explained by a curve that represents the top of extremely slim bins.

This smooth curve represents a \term{probability density function} (also called a \term{density} or \term{distribution}), and such a curve is shown in Figure~\ref{fdicHeightContDist} overlaid on a histogram of the sample. A density has a special property: the total area under the density's curve is 1. 
\begin{figure}[tbh]
\centering
\includegraphics[width=0.75\textwidth]{02/figures/fdicHeightContDist/fdicHeightContDist}
\caption{The continuous probability distribution of heights for US adults.}
\label{fdicHeightContDist}
\end{figure}

\subsection{Probabilities from continuous distributions}

We computed the proportion of individuals with heights \resp{180} to \resp{185} cm in Example~\exam{contDistProb} as a fraction:
\begin{eqnarray*}
\frac{\text{number of people between \resp{180} and \resp{185}}}{\text{total sample size}}
\end{eqnarray*}
We found the number of people with heights between \resp{180} and \resp{185} cm by determining the shaded boxes in this range, which represented the fraction of the box area in this region. Similarly, we can use the area in the shaded region under the curve to find a probability (with the help of a computer):
\begin{eqnarray*}
P(\text{\var{height} between \resp{180} and \resp{185}})
	= \text{area between \resp{180} and \resp{185}}
	= 0.1157
\end{eqnarray*}
The probability that a randomly selected person is between \resp{180} and \resp{185} cm is 0.1157. This is very close to the estimate from Example~\exam{contDistProb}: 0.1172. 
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{02/figures/fdicHeightContDistFilled/fdicHeightContDistFilled}
\caption{Density for heights in the US adult population with the area between 180 and 185 cm shaded. Compare this plot with Figure~\ref{usHeightsHist180185}.}
\label{fdicHeightContDistFilled}
\end{figure}

\begin{exercise}
Three US adults are randomly selected. The probability a single adult is between \resp{180} and \resp{185} cm is 0.1157. Short answers are in the footnote\footnote{(a) $0.1157 * 0.1157 * 0.1157 = 0.0015$. (b) $(1-0.1157)^3 = 0.692$}. \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] What is the probability that all three are between \resp{180} and \resp{185} cm tall?
\item[(b)] What is the probability that none are between \resp{180} and \resp{185} cm?
\end{enumerate}
\end{exercise}

\begin{exercise}\label{probabilityOfExactly180cm}
What is the probability a randomly selected person is \textbf{exactly} \resp{180} cm? Assume you can measure perfectly. Answer in the footnote\footnote{This probability is zero. A person might be close to \resp{180} cm, but not exactly \resp{180} cm tall. This also makes sense with the definition of probability as area; there is no area between \resp{180} cm to \resp{180} cm.}.
\end{exercise}

\begin{exercise}
Suppose a person's height is rounded to the nearest centimeter. Is there a chance that a random person's \textbf{measured} height will be \resp{180} cm? Answer in the footnote\footnote{This has positive probability. Anyone between \resp{179.5} cm and \resp{180.5} cm will have a \emph{measured} height of \resp{180} cm. This is probably a more realistic scenario to encounter in practice versus Exercise~\exer{probabilityOfExactly180cm}.}.
\end{exercise}

%%%%%%%%%%%%%%%%
\section{Conditional probability (special topic)}
\label{conditionalProbabilitySection}

Are students more likely to use marijuana when their parents used drugs? The \data{drugUse}\footnote{Ellis GJ and Stone LH. 1979. Marijuana Use in College: An Evaluation of a Modeling Explanation. Youth and Society 10:323-334.} data set contains a sample of 445 cases with two variables, \var{student} and \var{parents}, and is summarized in Table~\ref{contTableOfParStDrugUse}. The \var{student} variable is either \resp{uses} or \resp{not}, where a student \var{uses} if she has recently used marijuana. The \var{parents} variable takes the value \resp{used} if at least one of the parents used drugs, including alcohol.\vspace{-1mm}

\begin{table}[ht]
\centering%\small
\begin{tabular}{ll rr r rr}
  && \multicolumn{2}{c}{\var{parents}} & \hspace{1cm} &  \\
  \cline{3-4}
	&& \resp{used} & \resp{not} & Total  \\
  \cline{2-5}
	& \resp{uses}     & 125 & 94 & 219 \\
\raisebox{1.5ex}[0pt]{\var{student}}	& \resp{not} \hspace{0.5cm} & 85 & 141 & 226   \\
  \cline{2-5}
	& Total & 210 & 235 & 445 \\
\end{tabular}
\caption{Contingency table summarizing the \data{drugUse} data set.}
\label{contTableOfParStDrugUse}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[height=1.9in]{02/figures/drugUseVenn/drugUseVenn}
\caption{A Venn diagram using boxes for the \data{drugUse} data set.}
\label{drugUseVenn}
\end{figure}

\vspace{-3mm}

\begin{example}{If at least one parent used drugs, what is the chance their child (\var{student}) {uses}?}
We will estimate this probability using the data. Of the 210 cases in this data set where \var{parents} = \resp{used}, 125 represent cases where \var{student} = \resp{uses}:
\begin{eqnarray*}
P(\text{\var{student} = \resp{uses} given \var{parents} = \resp{used}}) = \frac{125}{210} = 0.60
\end{eqnarray*}
%For the cases in this data set, the probability is 0.60.
\end{example}

\begin{example}{A student is randomly selected from the study and she does not use drugs. What is the probability at least one of her parents used?}\label{drugUseProbOfParentsGivenStudents}
If the student does not use drugs, then she is one of the 226 students in the second row. Of these 226 students, 85 had at least one parent who used drugs:
\begin{eqnarray*}
P(\text{\var{parents} = \resp{used} given \var{student} = \resp{not}}) = \frac{85}{226} = 0.376
\end{eqnarray*}
\end{example}

\subsection{Marginal and joint probabilities}
\label{marginalAndJointProbabilities}

Table~\ref{contTableOfParStDrugUse} includes row and column totals for the counts. These totals provide summary information about each variable separately. These totals represent \term{marginal probabilities} for the sample, which are the probabilities based on a single variable without conditioning on any other variables. For instance, a probability based solely on the \var{student} variable is a marginal probability:
\begin{eqnarray*}
P(\text{\var{student} = \resp{uses}}) = \frac{219}{445} = 0.492
\end{eqnarray*}
A probability of outcomes for two or more variables or processes is called a \term{joint probability}:
\begin{eqnarray*}
P(\text{\var{student} = \resp{uses} \& \var{parents} = \resp{not}}) = \frac{94}{445} = 0.21
\end{eqnarray*}
It is common to substitute a comma for the ampersand (\&) in a joint probability, although either is acceptable. 
\begin{table}
\centering
\begin{tabular}{l rr r}
  \hline
& \var{parents}: \resp{used} & \var{parents}: \resp{not} & Total  \\
  \hline
\var{student}: \resp{uses}     & 0.28 & 0.21 & 0.49 \\
\var{student}: \resp{not} \hspace{0.5cm} & 0.19 & 0.32 & 0.51  \\
   \hline
Total & 0.47 & 0.53 & 1.000 \\
\hline
\end{tabular}
\caption{Probability table summarizing parental and student drug use.}
\label{drugUseProbTable}
\end{table}

\begin{termBox}{\tBoxTitle{Marginal and joint probabilities}
If a probability is based on a single variable, it is a \emph{marginal probability}. The probability of outcomes for two or more variables or processes is called a \emph{joint probability}.}
\end{termBox}

We use \term{table proportions} to summarize joint probabilities for the \data{drugUse} sample. These proportions are computed by dividing each count in Table~\ref{contTableOfParStDrugUse} by 445 to obtain the proportions in Table~\ref{drugUseProbTable}. The joint probability distribution of the \var{parents} and \var{student} variables is shown in Table~\ref{drugUseDistribution}. 
\begin{table}
\centering
\begin{tabular}{l c}
  \hline
Joint outcome & Probability \\
  \hline
\var{parents} = \resp{used}, \var{student} = \resp{uses} & 0.28 \\
\var{parents} = \resp{used}, \var{student} = \resp{not} & 0.19 \\
\var{parents} = \resp{not}, \var{student} = \resp{uses} & 0.21 \\
\var{parents} = \resp{not}, \var{student} = \resp{not} & 0.32 \\
   \hline
Total & 1.00 \\
\hline
\end{tabular}
\caption{A joint probability distribution for the \data{drugUse} data set.}
\label{drugUseDistribution}
\end{table}

\begin{exercise}
Verify Table~\ref{drugUseDistribution} represents a probability distribution: events are disjoint, all probabilities are non-negative, and the probabilities sum to 1.
\end{exercise}

\begin{exercise}
Which table do you find more useful, Table~\ref{drugUseProbTable} or Table~\ref{drugUseDistribution}?
\end{exercise}

We can compute marginal probabilities using joint probabilities in simple cases. For example, the probability a random student from the study uses drugs is found by summing the outcomes from Table~\ref{drugUseDistribution} where \var{student} = \resp{uses}:
\begin{eqnarray*}
&&P(\text{\underline{\color{highlight}\var{student} = \resp{uses}}}) \\
&& \quad =  P(\text{\var{parents} = \resp{used}, \underline{\color{highlight}\var{student} = \resp{uses}}}) + \\
&& \quad \quad \quad \quad P(\text{\var{parents} = \resp{not}, \underline{\color{highlight}\var{student} = \resp{uses}}}) \\
&& \quad = 0.28 + 0.21 = 0.49
\end{eqnarray*}

%\begin{exercise}
%Compute the probability that neither parent used drugs: \vspace{-1.5mm}
%\begin{align*}
%P(\text{\underline{\var{parents} = \resp{not}} \& \var{student} = \resp{uses}}) + P(\text{\underline{\var{parents} = \resp{not}} \& \var{student} = \resp{not}})
%\end{align*}
%\end{exercise}

%\begin{exercise}
%Using Table~\ref{drugUseProbTable}, identify the probability at least one parent used drugs in a random case from the study?
%\end{exercise}

\subsection{Defining conditional probability}

There is some connection between drug use of parents and of the student: drug use of one is associated with drug use of the other\footnote{This is an observational study and no causal conclusions may be reached.}. In this section, we discuss how to use information about associations between two variables to improve probability estimation.

The probability that a random student from the study uses drugs is 0.49. Could we update this probability if we knew that this student's parents used drugs? Absolutely. To do so, we limit our view to only those 210 cases where parents used drugs and look at the fraction where the student uses drugs:
\begin{eqnarray*}
P(\text{\var{student} = \resp{uses} given \var{parents} = \resp{used}}) = \frac{125}{210} = 0.60
\end{eqnarray*}
We call this a \term{conditional probability} because we computed the probability under a condition: \var{parents} = \resp{used}. There are two parts to a conditional probability, \term{the outcome of interest} and the \term{condition}. It is useful to think of the condition as information we know to be true, and this information usually can be described as a known outcome or event.

We separate the text inside our probability notation into the outcome of interest and the condition:
\begin{eqnarray}
&& P(\text{\var{student} = \resp{uses} given \var{parents} = \resp{used}}) \notag \\
&& = P(\text{\var{student} = \resp{uses} } | \text{ \var{parents} = \resp{used}}) = \frac{125}{210} = 0.60
\label{probStudentUsedIfParentsUsedInFormalNotation}
\end{eqnarray}
\marginpar[\raggedright\vspace{-10mm}

$P(A | B)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$\\given $B$]{\raggedright\vspace{-10mm}

$P(A | B)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$\\given $B$}The vertical bar ``$|$'' is read as \emph{given}.

In Equation~(\ref{probStudentUsedIfParentsUsedInFormalNotation}), we computed the probability a student uses based on the condition that at least one parent used as a fraction:
\begin{eqnarray}
&& P(\text{\var{student} = \resp{uses} } | \text{ \var{parents} = \resp{used}}) \notag \\
&&\quad = \frac{\text{\# times \var{student} = \resp{uses} \& \var{parents} = \resp{used}}}{\text{\# times \var{parents} = \resp{used}}} \label{ratioOfBothToRatioOfConditionalForParentsAndStudent} \\
&&\quad = \frac{125}{210} = 0.60 \notag
\end{eqnarray}
We considered only those cases that met the condition, \var{parents} = \resp{used}, and then we computed the ratio of those cases that satisfied our outcome of interest, that the student uses.

Counts are not always available for data, and instead only marginal and joint probabilities may be provided. Disease rates are commonly listed in percentages rather than in a count format. We would like to be able to compute conditional probabilities even when no counts are available, and we use Equation~(\ref{ratioOfBothToRatioOfConditionalForParentsAndStudent}) as an example demonstrating this technique.

We considered only those cases that satisfied the condition, \var{parents} = \resp{used}. Of these cases, the conditional probability was the fraction who represented the outcome of interest, \var{student} = \resp{uses}. Suppose we were provided only the information in Table~\vref{drugUseProbTable}, i.e. only probability data. Then if we took a sample of 1000 people, we would anticipate about 47\% or $0.47*1000 = 470$ would meet our information criterion. Similarly, we would expect about 28\% or $0.28*1000 = 280$ to meet both the information criterion and represent our outcome of interest. Thus, the conditional probability could be computed:
\begin{align}
P(\text{\var{student} = \resp{uses} } | \text{ \var{parents} = \resp{used}})
	&= \frac{\text{\# (\var{student} = \resp{uses} \& \var{parents} = \resp{used})}}{\text{\# (\var{parents} = \resp{used})}} \notag \\
	&= \frac{280}{470} = \frac{0.28}{0.47} = 0.60
\label{stUserPUsedHypSampSize}
\end{align}
where \var{S} and \var{parents} represent the \var{student} and \var{parents} variables. In Equation~(\ref{stUserPUsedHypSampSize}), we examine exactly the fraction of two probabilities, 0.28 and 0.47, which we can write as
\begin{align*}
P(\var{S} = \resp{uses}\ \&\ \var{parents} = \resp{used})
	\quad\text{and}\quad
	P(\var{parents} = \resp{used}).
\end{align*}
The fraction of these probabilities represents our general formula for conditional probability.

\begin{termBox}{\tBoxTitle{Conditional Probability}
The conditional probability of the outcome of interest $A$ given condition $B$ is computed as the following: \vspace{-1.5mm}
\begin{eqnarray}
P(A | B) = \frac{P(A\text{ \textbf{\&} }B)}{P(B)}
\label{condProbEq}
\end{eqnarray}}
\end{termBox}

\vspace{-9mm}

\begin{exercise}\label{drugUseProbOfParentsEqualNotGivenStudents}
Answers in the footnote\footnote{(a) $P(\text{\var{parent} = \resp{not}} | \text{\var{student} = \resp{not}})$. (b) Equation~(\ref{condProbEq}) for conditional probability suggests we should first find $P(\text{\var{parents} = \resp{not} \& \var{student} = \resp{not}})=0.32$ and $P(\text{\var{student} = \resp{not}})=0.51$. Then the ratio represents the conditional probability: $0.32/0.51 = 0.63$.}.\vspace{-1.5mm}
\begin{enumerate}
\item[(a)] Write out the following statement in conditional probability notation: ``\emph{The probability a random case has \var{parents} = \resp{not} if it is known that \var{student} = \resp{not}}''.  Notice that the condition is now based on the {student}, not the {parent}.
\item[(b)] Determine the probability from part (a). Table~\vref{drugUseProbTable} may be helpful.
\end{enumerate}
\end{exercise}

\begin{exercise}\label{whyCondProbSumTo1} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] Determine the probability that one of the parents had used drugs if it is known the student does not use drugs. %, i.e. \vspace{-1.5mm}
%\begin{eqnarray*}
%P(\text{\var{parents} = \resp{used} }|\text{\var{student} = \resp{not}})
%\end{eqnarray*}
\item[(b)] Using the answers from part (a) and Exercise~\exer{drugUseProbOfParentsEqualNotGivenStudents}(b), compute \vspace{-1.5mm}
\begin{eqnarray*}
P(\text{\var{parents} = \resp{used}}|\text{\var{student} = \resp{not}})
	+ P(\text{\var{parents} = \resp{not}}|\text{\var{student} = \resp{not}})
\end{eqnarray*}
\setlength{\itemsep}{-1.5mm}
\item[(c)] Provide an intuitive argument to explain why the sum in (b) is 1. Answer in the footnote\footnote{Under the condition the student does not use drugs, the parents must either use drugs or not. The complement still appears to work \emph{when conditioning on the same information}.}. \vspace{-1.5mm}
\end{enumerate}
\end{exercise}

\begin{exercise}
The data indicate that drug use of parents and children are associated. Does this mean the drug use of parents causes the drug use of the students? Answer in the footnote\footnote{No. This was an observational study. Two potential confounding variables include \var{income} and \var{region}. Can you think of others?}.
\end{exercise}

\subsection{Smallpox in Boston, 1721}

The \data{smallpox} data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in Boston\footnote{Fenner F. 1988. \emph{Smallpox and Its Eradication (History of International Public Health, No. 6)}. Geneva: World Health Organization. ISBN 92-4-156110-6.}. Doctors at the time believed that inoculation, which involves exposing a person to the disease in a controlled form, could reduce the likelihood of death.

Each case represents one person with two variables: \var{inoculated} and \var{result}. The variable \var{inoculated} takes two levels: \resp{yes} or \resp{no}, indicating whether the person was inoculated or not. The variable \var{result} has outcomes \resp{lived} or \resp{died}. We summarize the data in Tables~\ref{smallpoxContingencyTable} and~\ref{smallpoxProbabilityTable}.
\begin{table}
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
\cline{2-5}
		& \resp{lived}     & 238 & 5136 & 5374 \\
\raisebox{1.5ex}[0pt]{\var{result}} &  \resp{died} \hspace{0.5cm} & 6 & 844 & 850  \\
\cline{2-5}
	& Total & 244 & 5980 & 6224 \\
\end{tabular}
\caption{Contingency table for the \data{smallpox} data set.}
\label{smallpoxContingencyTable}
\end{table}
\begin{table}
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
   \cline{2-5}
 & \resp{lived}     & 0.0382 & 0.8252 & 0.8634 \\
\raisebox{1.5ex}[0pt]{\var{result}} & \resp{died} \hspace{0.5cm} & 0.0010 & 0.1356  & 0.1366  \\
   \cline{2-5}
& Total & 0.0392 & 0.9608 & 1.0000 \\
\end{tabular}
\caption{Table proportions for the \data{smallpox} data, computed by dividing each count by the table total, 6224.}
\label{smallpoxProbabilityTable}
\end{table}

\begin{exercise} \label{probDiedIfNotInoculated}
Write out, in formal notation, the probability a randomly selected person who was not inoculated died from smallpox, and find this probability. Brief answer in the footnote\footnote{$P($\var{result} = \resp{died} $|$ \var{inoculated} = \resp{no}$) = \frac{0.1356}{0.9608} = 0.1411$.}.
\end{exercise}

\begin{exercise}
Determine the probability an inoculated person died from smallpox. How does this result compare with the result of Exercise~\exer{probDiedIfNotInoculated}?
\end{exercise}

\begin{exercise}
The people of Boston self-selected whether or not to be inoculated. (a) Is this study observational or experimental? (b) Can we infer any causal connection using these data? (c) What are some potential confounding variables that might influence whether someone \resp{lived} or \resp{died} and affect whether that person was inoculated or not? Answers in the footnote\footnote{(a) Observational. (b) No! We cannot infer causation from this observational study. (c) Accessibility to the latest and best medical care. (There are other valid answers.)}.
\end{exercise}

\subsection{General multiplication rule}

Section~\ref{probabilityIndependence} introduced a multiplication rule for independent processes. Here we provide a General Multiplication Rule for events that might not be independent.

\begin{termBox}{\tBoxTitle{General Multiplication Rule}
If $A$ and $B$ represent two outcomes or events, then \vspace{-1.5mm}
\begin{eqnarray*}
P(A\text{ and }B) = P(A | B)*P(B)
\end{eqnarray*} \vspace{-6.5mm} \par
It is useful to think of $A$ as the outcome of interest and $B$ as the condition.}
\end{termBox}
This General Multiplication Rule is simply a rearrangement of the definition for conditional probability in Equation~(\ref{condProbEq}) on page~\pageref{condProbEq}.

\begin{example}{Consider the \data{smallpox} data set. Suppose we knew that 85.88\% of residents exposed to smallpox were not inoculated, 96.08\% of exposed residents were not inoculated, and no other information was provided. How could we compute the probability a resident who was exposed to smallpox was not inoculated and also survived?}
We will compute our answer and then verify it using Table~\ref{smallpoxProbabilityTable}. We want to determine
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} \& \var{inoculated} = \resp{no}})
\end{eqnarray*}
and we are given that
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} }|\text{ \var{inoculated} = \resp{no}})=0.8588 \\
P(\text{\var{inoculated} = \resp{no}})=0.9608
\end{eqnarray*}
Among the 96.08\% of people who were not inoculated, 85.88\% survived:
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} \& \var{inoculated} = \resp{no}}) = 0.8588*0.9608 = 0.8251
\end{eqnarray*}
This is equivalent to the General Multiplication Rule. We can confirm this probability in Table~\vref{smallpoxProbabilityTable} at the intersection of \resp{no} and \resp{lived} (with a little rounding error).
\end{example}

\begin{exercise}
Use $P($\var{inoculated} = \resp{yes}$) = 0.0392$ and $P($\var{result} = \resp{lived} $|$ \var{inoculated} = \resp{yes}$) = 0.9754$ to determine the probability a person was both inoculated and lived. Verify your answer using Table~\ref{smallpoxProbabilityTable}.
\end{exercise}

\begin{termBox}{\tBoxTitle{Sum of conditional probabilities}
Let $A_1$, ..., $A_k$ represent all the disjoint outcomes for a variable or process. Then if $B$ is a condition for another variable or process, we have: \vspace{-1.5mm}
\begin{eqnarray*}
P(A_1|B)+\cdots+P(A_k|B) = 1
\end{eqnarray*}
The rule for complements also holds when an event and its complement are conditioned on the same information: \vspace{-1.5mm}
\begin{eqnarray*}
P(A | B) = 1 - P(A^c | B)
\end{eqnarray*}}
\end{termBox}

\begin{exercise}
If 97.54\% of the people who were inoculated lived, what proportion of inoculated people must have died?
\end{exercise}

\begin{exercise}
Do you think inoculation is effective at reducing the risk of death from smallpox? Use Table~\ref{smallpoxContingencyTable} to support your answer.
\end{exercise}

\subsection{Independence considerations in conditional probability}

If two processes are independent, then knowing the outcome of one should provide no information about the other. We can show this is mathematically true using conditional probabilities.

\begin{exercise} \label{condProbOfRollingA1AfterOne1}
Let $X$ and $Y$ represent the outcomes of rolling two dice. (a) What is the probability the first die, $X$, is \resp{1}? (b) What is the probability both $X$ and $Y$ are \resp{1}? (c) Use the formula for conditional probability to compute $P(Y =$ \resp{1} $| X = $ \resp{1}$)$. (d) When $X$ was conditioned to be \resp{1}, did it alter the probability $Y$ was \resp{1} in part (c)?
\end{exercise}

We can show in Exercise~\exer{condProbOfRollingA1AfterOne1}(c) that the conditioning information has no influence by using the Multiplication Rule for independence processes:
\begin{eqnarray*}
P(Y=\text{\resp{1}}|X=\text{\resp{1}})
	&=& \frac{P(Y=\text{\resp{1} \& }X=\text{\resp{1}})}{P(X=\text{\resp{1}})} \\
	&=& \frac{P(Y=\text{\resp{1}})*\color{tableHLBlue}P(X=\text{\resp{1}})}{\color{tableHLBlue}P(X=\text{\resp{1}})} \\
	&=& P(Y=\text{\resp{1}}) \\
\end{eqnarray*}

\begin{exercise}
Ron is watching a roulette table in a casino and notices that the last five outcomes were \resp{black}. He figures that the chances of getting \resp{black} six times in a row is very small (about $1/64$) and puts his paycheck on red. What is wrong with his reasoning?
\end{exercise}

\subsection{Tree diagrams}

\termsub{Tree diagrams}{tree diagram} are a tool to organize outcomes and probabilities around the structure of the data. They are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors.

The \data{smallpox} data fit this description. We see the population as split by \var{inoculation}: \resp{yes} and \resp{no}. Following this split, survival rates were observed for each group. We construct the tree diagram to follow the data structure. It splits the group by \var{inoculation} and then split those subgroups by \var{result}, shown in Figure~\ref{smallpoxTreeDiagram}. The first branch for \var{inoculation} is said to be the \term{primary} branch while the other branches are \term{secondary}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.93\textwidth]{02/figures/smallpoxTreeDiagram/smallpoxTreeDiagram}
\caption{A tree diagram of the \data{smallpox} data set.}
\label{smallpoxTreeDiagram}
\end{figure}

We annotate the tree diagram with marginal and conditional probabilities on the branches. We first split the data by \var{inoculation} into the \resp{yes} and \resp{no} groups with respective marginal probabilities 0.0392 and 0.9608. The second split is conditioned on the first, so we assign conditional probabilities to the branches. For example, the top branch in Figure~\ref{smallpoxTreeDiagram} is the probability that \var{result} = \resp{lived} conditioned on the information that \var{inoculated} = \resp{yes}. We may (and usually do) construct joint probabilities at the end of each branch in our tree by multiplying the numbers we come across as we move from left to right. These joint probabilities are computed directly from the General Multiplication Rule:
\begin{eqnarray*}
&& P(\text{\var{inoculated} = \resp{yes} \& \var{result} = \resp{lived}}) \\
	&&\quad = P(\text{\var{inoculated} = \resp{yes}})*P(\text{\var{result} = \resp{lived}}|\text{\var{inoculated} = \resp{yes}}) \\
	&&\quad = 0.0392*0.9754=0.0382
\end{eqnarray*}

\begin{example}{Consider the midterm and final for a statistics class. Suppose 13\% of students earn an \resp{A} on the midterm. Of those students who earned an \resp{A} on the midterm, 47\% got an \resp{A} on the final, and 11\% of the students who got lower than an \resp{A} on the midterm got an \resp{A} on the final. You randomly pick up a final exam and notice the student got an \resp{A}. What is the probability this student got an \resp{A} on the midterm?} \label{exerciseForTreeDiagramOfStudentGettingAOnMidtermGivenThatSheGotAOnFinal}
It is not obvious how to solve this problem. However, we can start by organizing our information into a tree diagram. First split the students based on the midterm and then split those primary branches into secondary branches based on the final. We associate the marginal probability with the primary branches and the conditional probabilities with the secondary branches. The result is shown in Figure~\ref{testTree}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{02/figures/testTree/testTree}
\caption{A tree diagram describing the \var{midterm} and \var{final} variables.}
\label{testTree}
\end{figure}

The end-goal is to find $P(\text{\var{midterm} = \resp{A}} | \text{\var{final} = \resp{A}})$. We can start by finding the two probabilities associated with this conditional probability using the tree diagram:
\begin{eqnarray*}
&&P(\text{\var{midterm} = \resp{A} \& \var{final} = \resp{A}}) = 0.0611 \\
&&P(\text{\var{final} = \resp{A}})  \\
&& \quad= P(\text{\var{midterm} = \resp{other} \& \var{final} = \resp{A}}) + P(\text{\var{midterm} = \resp{A} \& \var{final} = \resp{A}}) \\
&& \quad= 0.0611 + 0.0957 = 0.1568 \\
\end{eqnarray*}
Then we take the ratio of these two probabilities:
\begin{eqnarray*}
P(\text{\var{midterm} = \resp{A}} | \text{\var{final} = \resp{A}}) &=& \frac{P(\text{\var{midterm} = \resp{A} \& \var{final} = \resp{A}})}{P(\text{\var{final} = \resp{A}})} \\
&=& \frac{0.0611}{0.1568} = 0.3897
\end{eqnarray*}
The probability the student also got an A on the midterm is about 0.39.
\end{example}

\begin{exercise}
After an introductory statistics course, 78\% of students can successfully construct tree diagrams. Of those who can construct tree diagrams, 97\% passed, while only 57\% of those students who could not construct tree diagrams passed. (a) Organize this information into a tree diagram. (b) What is the probability a randomly selected student passed? (c) Compute the probability a student is able to construct a tree diagram if it is known that she passed. Hints plus a short answer to (c) in the footnote\footnote{(a) The first branch should be based on the variable that directly splits the data into groups: whether students can construct tree diagrams. (b) Identify which two joint probabilities represent students who passed. (c) Use the definition of conditional probability. Your answer to part (b) may be useful in part (c). The solution to (c) is 0.86.}.
\end{exercise}

%An additional tree diagram example (Example~\ref{probabilityOfLupusGivenPositiveTestExample}) is given in the next subsection.

\subsection{Bayes' Theorem}

In many instances, we are given a conditional probability of the form
\begin{align*}
P(\text{statement about variable 1 } | \text{ statement about variable 2})
\end{align*}
but we would really like to know the inverted conditional probability:
\begin{align*}
P(\text{statement about variable 2 } | \text{ statement about variable 1})
\end{align*}
Tree diagrams can sometimes be used to find the second conditional probability when given the first. However, sometimes it is not always possible to draw the scenario in a tree diagram. In these cases, we can apply a very useful and general formula: Bayes' Theorem.

We first take a critical look at a new example of inverting conditional probabilities where we still apply a tree diagram.

\begin{example}{In the US, 0.5\% of the population has Lupus. Suppose we consider a new test for Lupus, but this test is not perfectly accurate. In 1\% of patients with Lupus, the test gives a \emph{false negative}: it indicates a person does not have Lupus when she does have Lupus. Similarly, the test gives a \emph{false positive} in 2\% of patients who do not have Lupus: it indicates these patients have Lupus when they actually do not. If we tested a random person for Lupus and the test came back positive -- that is, the test indicated the patient has Lupus -- what is the probability the person actually has Lupus?} \label{probabilityOfLupusGivenPositiveTestExample}
\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{02/figures/lupusAndPositiveTestTreeDiagram/lupusAndPositiveTestTreeDiagram}
\caption{Tree diagram for Example~\ref{probabilityOfLupusGivenPositiveTestExample}, computing the probability a random patient who tests positive for Lupus actually has Lupus.}
\label{lupusAndPositiveTestTreeDiagram}
\end{figure}
This is a conditional probability, and it can be broken into two pieces:\vspace{-1.5mm}
\begin{align*}
P(\text{patient has Lupus } | \text{pos. test}) = \frac{P(\text{patient has Lupus and the test is positive})}{P(\text{pos. test})}
\end{align*}
A tree diagram is useful for identifying each probability and is shown in Figure~\ref{lupusAndPositiveTestTreeDiagram}. The probability the patient has Lupus and has a positive test result is\vspace{-1.5mm}
\begin{align*}
P(\text{pos. test \& has Lupus}) &= P(\text{pos. test } | \text{ has Lupus})P(\text{has Lupus}) \\
	&= 0.99*0.005 = 0.00495
\end{align*}
The probability of a positive test result is the sum of the two corresponding scenarios:\vspace{-1.5mm}
\begin{align*}
P(\text{pos. test}) &= P(\text{pos. test \& no Lupus}) + P(\text{pos. test \& has Lupus}) \\
	&= P(\text{pos. test } | \text{ no Lupus})P(\text{no Lupus}) \\
			   &\qquad + P(\text{pos. test } | \text{ has Lupus})P(\text{has Lupus}) \\
	&= 0.02*0.995 + 0.99*0.005 = 0.02485
\end{align*}
Then the probability the patient has Lupus conditioned on the test result being positive is
\begin{align*}
P(\text{patient has Lupus } | \text{pos. test}) = \frac{0.00495}{0.02485} \approx 0.1992
\end{align*}
That is, even if a patient has a positive test result, there is still only a 20\% chance she has Lupus.
\end{example}

Consider the conditional probability of Example~\ref{probabilityOfLupusGivenPositiveTestExample}:
\begin{align*}
P(\text{patient has Lupus } | \text{pos. test}) = \frac{P(\text{patient has Lupus and the test is positive})}{P(\text{the test is positive})}
\end{align*}
Using the tree diagram, we can see that the numerator (the top of the fraction) is equal to the following product:
\begin{align*}
P(\text{pos. test \& has Lupus}) = P(\text{pos. test } | \text{ has Lupus})P(\text{has Lupus})
\end{align*}
The denominator -- the probability of a positive test result -- is equal to all the different ways the test could be positive:
\begin{align*}
P(\text{pos. test}) &= P(\text{pos. test \& no Lupus}) + P(\text{pos. test \& has Lupus}) %\\
\end{align*}
Each of these probabilities can be broken down into a product of a conditional and marginal probability, just like what was done in the numerator:
\begin{align*}
P(\text{pos. test}) &= P(\text{pos. test \& no Lupus}) + P(\text{pos. test \& has Lupus}) \\
	&= P(\text{pos. test } | \text{ no Lupus})P(\text{no Lupus}) \\
			   &\qquad + P(\text{pos. test } | \text{ has Lupus})P(\text{has Lupus})
\end{align*}
This example, where the numerator and the denominator are broken down into their pieces, actually shows a particular case of Bayes' Theorem.
\begin{align*}
& P(\text{patient has Lupus } | \text{pos. test})  \\
& \quad= \frac{P(\text{pos. test } | \text{ Lupus})P(\text{Lupus})}
	{P(\text{pos. test } | \text{ no Lupus})P(\text{no Lupus}) + P(\text{pos. test } | \text{ Lupus})P(\text{Lupus})}
\end{align*}

\begin{termBox}{\tBoxTitle{Bayes' Theorem: inverting probabilities}
Consider the following conditional probability for variable 1 and variable 2:\vspace{-1.5mm}
\begin{align*}
P(\text{outcome $A_1$ of variable 1 } | \text{ outcome $B$ of variable 2})
\end{align*}
Bayes' Theorem states that this conditional probability can be identified as the following fraction:\vspace{-1.5mm}
\begin{align}
\frac{P(B | A_1)*P(A_1)}
	{P(B | A_1)*P(A_1) + P(B | A_2)*P(A_2) + \cdots + P(B | A_k)*P(A_k)}
	\label{equationOfBayesTheorem}
\end{align}
where $A_2$, $A_3$, ..., and $A_k$ represent all other possible outcomes of the first variable.}
\end{termBox}

Bayes' Theorem is just a generalization of what we have done using tree diagrams. The numerator (top of the fraction) identifies the probability of getting both $A_1$ and $B$. The denominator is the marginal probability of getting $B$. This second part looks long and complicated since we have to add up probabilities from all the different ways to get $B$. We always had to complete this step when using tree diagrams. However, we usually did it in a separate step so it didn't seem as complex.

To apply Bayes' Theorem correctly, there are two preparatory steps:
\begin{enumerate}
\item[(1)] First identify the marginal probabilities of each possible outcome of the first variable: $P(A_1)$, $P(A_2)$, ..., $P(A_k)$.
\item[(2)] Then identify the probability of the outcome $B$, conditioned on each possible scenario for the first variable: $P(B | A_1)$, $P(B | A_2)$, ..., $P(B | A_k)$.
\end{enumerate}
Once each of these probabilities is identified, they can be applied directly within the formula.

\begin{tipBox}{\tipBoxTitle{Only use Bayes' Theorem when tree diagrams are difficult}
Drawing a tree diagram makes it easier to understand how two variables are connected. Use Bayes' Theorem only when there are so many scenarios that drawing a tree diagram would be complex.}
\end{tipBox}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent}
Jose visits campus every Thursday evening. However, some days the parking garage is full, often due to events. There are academic events on 35\% of evenings and sporting events on 20\% of evenings. When there is an academic event, the garage fills up about 25\% of the time, and it fills up 70\% of evenings with sporting events. On evenings when there are no events, it only fills up about 5\% of the time. If Jose comes to campus and finds the garage full, what is the probability that there is a sporting event? Solve this problem using a tree diagram. Hint about the diagram in the footnote\footnote{This tree diagram will have three primary branches: there is an academic event, sporting event, or no event on campus that evening.}.
\end{exercise}

\begin{example}{Here we solve Exercise~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent} using Bayes' Theorem.}
The outcome of interest is whether there is a sporting event (call this $A_1$), and the condition is that the lot is full ($B$). Let $A_2$ represent an academic event and $A_3$ represent there being no event on campus. Then the given probabilities can be written as\vspace{-1.5mm}
\begin{align*}
P(A_1) &= 0.2 &P(A_2) &= 0.35 &P(A_3) &= 1-0.2-0.35 = 0.45 \\
P(B | A_1) &= 0.7 &P(B | A_2) &= 0.25 &P(B | A_3) &= 0.05
\end{align*}
Bayes' Theorem can be used to compute the probability of a sporting event ($A_1$) under the condition that the parking lot is full ($B$):\vspace{-1.5mm}
\begin{align*}
P(A_1 | B) &= \frac{P(B | A_1)*P(A_1)}{P(B | A_1)*P(A_1) + P(B | A_2)*P(A_2) + P(B | A_3)*P(A_3)} \\
		&= \frac{(0.7)(0.2)}{(0.7)(0.2) + (0.25)(0.35) + (0.05)(0.45)} \\
		&= 0.56 
\end{align*}
While we have greater reason to believe there is a sporting event, we are by no means certain.
\end{example}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsAnAcademicEvent}
Using the probabilities given in Exercise~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent}, verify the probability that there is an academic event conditioned on the parking lot being full is 0.35.
\end{exercise}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsNoEvent}
In Exercise~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent} and~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsAnAcademicEvent}, you found that if the parking lot is full, the probability a sporting event is 0.56 and the probability there is an academic event is 0.35. Using this information, compute $P(\text{no event } | \text{ the lot is full})$?
\end{exercise}

%Exercises~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent}, \ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsAnAcademicEvent}, and \ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsNoEvent} 
The last several exercises offered a way to update how much we might believe there is a sporting event, academic event, or no event going on at the school based on the information that the parking lot was full. This strategy of \emph{updating beliefs} using Bayes' Theorem is actually the foundation of an entire section of statistics called \term{Bayesian statistics}. While Bayesian statistics is very important and useful, we will not have time to cover much more of it in this book.

%%%%%%%%%
\section{Sampling from a small population (special topic)}
\label{smallPop}

\begin{example}{Professors sometimes select a student at random to answer a question. If the selection is truly random, and there are 15 people in your class, what is the chance that she will pick you for the next question?}
If there are 15 people to ask and none are skipping class (for once), then the probability is $1/15$, or about $0.067$.
\end{example}

\begin{example}{If the professor asks 3 questions, what is the probability you will not be selected? Assume that she will not pick the same person twice in a given lecture.}\label{3woRep}
For the first question, she will pick someone else with probability $14/15$. When she asks the second question, she only has 14 people who have not yet been asked. Thus, if you were not picked on the first question, the probability you are again not picked is $13/14$. Similarly, the probability you are again not picked on the third question is $12/13$, and the probability of not being picked for any of the three questions is
\begin{eqnarray*}
&&P(\text{not picked in 3 questions}) \\
&&\quad = P(\text{\var{Q1}} = \text{\resp{notPicked}, }\text{\var{Q2}} = \text{\resp{notPicked}, }\text{\var{Q3}} = \text{\resp{notPicked}.}) \\
&&\quad = \frac{14}{15}\times\frac{13}{14}\times\frac{12}{13} = \frac{12}{15} = 0.80
\end{eqnarray*}
\end{example}

\begin{exercise}
What rule permitted us to multiply the probabilities in Example~\exam{3woRep}? Answer in the footnote\footnote{The three probabilities we computed were actually one marginal probability, $P($\var{Q1}$ = $\resp{notPicked}$)$, and two conditional probabilities:
\begin{eqnarray*}
&&P(\text{\var{Q2}} =  \text{\resp{notPicked}}|\text{\var{Q1}} = \text{\resp{notPicked}}) \\
&&P(\text{\var{Q3}} =  \text{\resp{notPicked}}|\text{\var{Q1}} = \text{\resp{notPicked}, }\text{\var{Q2}} = \text{\resp{notPicked}})
\end{eqnarray*}
Using the General Multiplication Rule, the product of these three probabilities is the probability of not being picked in 3 questions.}.
\end{exercise}

\begin{example}{Suppose the professor randomly picks without regard to who she already selected, i.e. students can be picked more than once. What is the probability you will not be picked for any of the three questions?}\label{3wRep}
Each pick is independent, and the probability of \resp{notPicked} on any one question is $14/15$. Thus, we can use the Product Rule for independent events.
\begin{eqnarray*}
&&P(\text{not picked in 3 questions}) \\
&&\quad = P(\text{\var{Q1}} = \text{\resp{notPicked}, }\text{\var{Q2}} = \text{\resp{notPicked}, }\text{\var{Q3}} = \text{\resp{notPicked}.}) \\
&&\quad = \frac{14}{15}\times\frac{14}{15}\times\frac{14}{15} = 0.813
\end{eqnarray*}
You have a slightly higher chance of not being picked compared to when she picked a new person for each question. However, you now may be picked more than once.
\end{example}

\begin{exercise}
Under the setup of Example~\exam{3wRep}, what is the probability of being picked to answer all three questions?
\end{exercise}

If we sample from a small population \term{without replacement}, we no longer have independence between our observations. In Example~\exam{3woRep}, the probability of \resp{notPicked} for the second question was conditioned on the event that you were \resp{notPicked} for the first question. In Example~\exam{3wRep}, the professor sampled her students \term{with replacement}: she repeatedly sampled the entire class without regard to who she already picked. 

\begin{exercise} \label{raffleOf30TicketsWWOReplacement}
Your department is holding a raffle. They sell 30 tickets and offer seven prizes. (a) They place the tickets in a hat and draw one for each prize. The tickets are sampled without replacement, i.e. the selected tickets are not placed back in the hat. What is the probability of winning a prize if you buy one ticket? (b) What if the tickets are sampled with replacement? Answers in the footnote\footnote{(a) First determine the probability of not winning. The tickets are sampled without replacement, which means the probability you do not win on the first draw is $29/30$, $28/29$ for the second, ..., and $23/24$ for the seventh. The probability you win no prize is the product of these separate probabilities: $23/30$. That is, the probability of winning a prize is $7/30 = 0.233$. (b) When the tickets are sampled with replacement, they are seven independent draws. Again we first find the probability of not winning a prize: $(29/30)^7 = 0.789$. Thus, the probability of winning (at least) one prize when drawing with replacement is 0.211.}.
\end{exercise}

\begin{exercise} \label{followUpToRaffleOf30TicketsWWOReplacement}
Compare your answers in Exercise~\exer{raffleOf30TicketsWWOReplacement}. How much influence does the sampling method have on your chances of winning a prize?
\end{exercise}

Had we repeated Exercise~\exer{raffleOf30TicketsWWOReplacement} with 300 tickets instead of 30, we would have found something interesting: the results would be nearly identical. The probability would be 0.0233 without replacement and 0.0231 with replacement. When the sample size is only a small fraction of the population (under 10\%), observations are nearly independent even when sampling without replacement.

%%%%%%%%%
\section{Random variables (special topic)}
\label{randomVariablesSection}

\begin{example}{Two books are assigned for a statistics class: a textbook and its corresponding study guide. The student bookstore determined 20\% of enrolled students do not buy either book, 55\% buy the textbook, and 25\% buy both books, and these percentages are relatively constant from one term to another. If there are 100 students enrolled, how many books should the bookstore expect to sell to this class?}\label{bookStoreSales}
Around 20 students will not buy either book (0 books total), about 55 will buy one book (55 total), and approximately 25 will buy two books (totaling 50 books for these 25 students). The bookstore should expect to sell about 105 books for this class.
\end{example}

\begin{exercise}
Would you be surprised if the bookstore sold slightly more or less than 105 books?
\end{exercise}

\begin{example}{The textbook costs \$137 and the study guide \$33. How much revenue should the bookstore expect from this class of 100 students?}\label{bookStoreRev}
About 55 students will just buy a textbook, providing revenue of
\begin{eqnarray*}
\$137 * 55 = \$7,535
\end{eqnarray*}
The roughly 25 students who buy both the textbook and a study guide would pay a total of
\begin{eqnarray*}
(\$137 + \$33) * 25 = \$170 * 25 = \$4,250
\end{eqnarray*}
Thus, the bookstore should expect to generate about \$11,785 from these 100 students for this one class. However, there might be some \emph{sampling variability} so the actual amount may differ by a little bit.
\end{example}
\begin{figure}
\centering
\includegraphics[width=0.69\textwidth]{02/figures/bookCostDist/bookCostDist}
\caption{Probability distribution for the bookstore's revenue from a single student. The distribution balances on a pyramid representing the average revenue per student.}
\label{bookCostDist}
\end{figure}

\begin{example}{What is the average revenue per student for this course?}\label{revFromStudent}
The expected total revenue is \$11,785, and there are 100 students. Therefore the expected revenue per student is $\$11,785/100 =  \$117.85$.
\end{example}

\subsection{Expectation}

We call a variable or process with a numerical outcome a \term{random variable}, and usually represent this random variable with a capital letter such as $X$, $Y$, or $Z$. The amount of money a single student will spend on her statistics books is a random variable, and we represent it by $X$.

\begin{termBox}{\tBoxTitle{Random variable}
A random process or variable with a numerical outcome.}
\end{termBox}

The possible outcomes of $X$ are labeled with a corresponding lower case letter and subscripts. For example, we write $x_1=\$0$, $x_2=\$137$, and $x_3=\$170$, which occur with probabilities $0.20$, $0.55$, and $0.25$. The distribution of $X$ is summarized in Figure~\ref{bookCostDist} and Table~\ref{statSpendDist}.
\begin{table}
\centering
\begin{tabular}{l ccc r}
\hline
$i$	  & 1 & 2 & 3  & Total\\
\hline
$x_i$ & \$0 & \$137 & \$170 & --\\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 & 1.00 \\
\hline
\end{tabular}
\caption{The probability distribution for the random variable $X$, representing the bookstore's revenue from a single student.}
\label{statSpendDist}
\end{table}

We computed the average outcome of $X$ as \$117.85 in Example~\exam{revFromStudent}. We call this average the \term{expected value} of $X$, denoted by $E(X)$\marginpar[\raggedright\vspace{-3mm}

$E(X)$\vspace{1mm}\\\footnotesize Expected\\value of $X$]{\raggedright\vspace{-3mm}

$E(X)$\vspace{1mm}\\\footnotesize Expected\\value of $X$}. The expected value of a random variable is computed by adding each outcome weighted by its probability:
\begin{align*}
E(X) &= \$0 * P(X=\$0) + \$137 * P(X=\$137) + \$170 * P(X=\$170) \\
	&= \$0 * 0.20 + \$137 * 0.55 + \$170 * 0.25 = \$117.85
\end{align*}

\begin{termBox}{\tBoxTitle{Expected value of a Discrete Random Variable}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$, the expected value of $X$ is the sum of each outcome multiplied by its corresponding probability:
\begin{align}
E(X) 	&= x_1*P(X=x_1) + \cdots + x_k*P(X=x_k) \notag \\
	&= \sum_{i=1}^{k}x_iP(X=x_i)
\end{align}
The notation $\mu$ may be used in place of $E(X)$.}
\end{termBox}

The expected value for a random variable represents the average outcome. For example, $E(X)=117.85$ represents the average amount the bookstore expects to make from a single student, which we could write as $\mu=117.85$.

It is also possible to compute the expected value of a continuous random variable. However, it requires a little calculus and we save it for a later class\footnote{$\mu = \int xf(x)dx$ where $f(x)$ represents a function for the density curve.}.

In physics, the expectation holds the same meaning as the center of gravity. The distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point. This is represented in Figures~\ref{bookCostDist} and~\ref{bookWts}. The idea of a center of gravity also expands to a continuous probability distribution. Figure~\ref{contBalance} shows a continuous probability distribution balanced atop a wedge placed at the mean.
\begin{figure}
\centering
\includegraphics[width=0.68\textwidth]{02/figures/bookWts/bookWts}
\caption{A weight system representing the probability distribution for $X$. The string holds the distribution at the mean to keep the system balanced.}
\label{bookWts}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.64\textwidth]{02/figures/contBalance/contBalance}
\caption{A continuous distribution can also be balanced at its mean.}
\label{contBalance}
\end{figure}

\subsection{Variability in random variables}

Suppose you ran the university bookstore. Besides how much revenue you expect to generate, you might also want to know the volatility (variability) in your revenue. 

The \indexthis{variance}{variance} and \indexthis{standard deviation}{standard deviation} can be used to describe the variability of a random variable. Section~1.3.5 % ZZQ \ref{variability}
introduced a method for finding the variance and standard deviation for a data set. We first computed deviations from the mean ($x_i - \mu$), squared those deviations, and took an average to get the variance. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation. This weighted sum of squared deviations equals the variance, and we calculate the standard deviation by taking the square root of the variance, just as we did in Section~1.3.5. % ZZQ \ref{variability}.

\begin{termBox}{\tBoxTitle{General variance formula}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$ and expected value $\mu=E(X)$, then the variance of $X$, denoted by $Var(X)$ or the symbol $\sigma^2$, is
\begin{align}
\sigma^2 &= (x_1-\mu)^2*P(X=x_1) + \cdots \notag \\
	& \qquad\quad\cdots+ (x_k-\mu)^2*P(X=x_k) \notag \\
	&= \sum_{j=1}^{k} (x_j - \mu)^2 P(X=x_j)
\end{align}
The standard deviation of $X$ ($\sigma$) is the square root of the variance.}
\end{termBox}
\marginpar[\raggedright\vspace{-47mm}

$Var(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$]{\raggedright\vspace{-47mm}

$Var(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$}

\begin{example}{Compute the expected value, variance, and standard deviation of $X$, the revenue of a single statistics student for the bookstore.}
It is useful to construct a table that holds computations for each outcome separately, then add up the results.
\begin{center}
\begin{tabular}{l rrr r}
\hline
$i$ & 1 & 2 & 3 & Total \\
\hline
$x_i$ & \$0 & \$137 & \$170 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i * P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
\hline
\end{tabular}
\end{center}
Thus, the expected value is $\mu=117.35$, which we computed earlier. The variance can be constructed by extending this table:
\begin{center}
\begin{tabular}{l rrr r}
\hline
$i$ & 1 & 2 & 3 & Total \\
\hline
$x_i$ & \$0 & \$137 & \$170 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i * P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
$x_i - \mu$ & -117.85 & 19.15 & 52.15 &  \\
$(x_i-\mu)^2$ & 13888.62 &  366.72 & 2719.62 &  \\
$(x_i-\mu)^2*P(X=x_i)$ & 2777.7 & 201.7 & 679.9 & 3659.3 \\
\hline
\end{tabular}
\end{center}
The variance of $X$ is $\sigma^2 = 3659.3$, which means the standard deviation is $\sigma = \sqrt{3659.3} = \$60.49$.
\end{example}

% ZZQ Formatting
\vspace{10mm}

\begin{exercise}
The bookstore also offers a chemistry textbook for \$159 and a book supplement for \$41. From past experience, they know about 25\% of chemistry students just buy the textbook while 60\% buy both the textbook and supplement. Answers for each part below are provided in the footnote\footnote{(a) 100\% - 25\% - 60\% = 15\% of students do not buy any books for the class. Part~(b) is represented by the first two lines in the table below. The expectation for part~(c) is given as the total on the line $y_i*P(Y=y_i)$. The result of part~(d) is the square-root of the variance listed on in the total on the last line: $\sigma = \sqrt{Var(Y)} = \$69.28$.
\begin{center}
\begin{tabular}{rrrrr}
  \hline
$i$ (scenario) & 1 (\resp{noBook}) & 2 (\resp{textbook}) & 3 (\resp{both}) & Total \\
  \hline
$y_i$ & 0.00 & 159.00 & 200.00 &  \\
$P(Y=y_i)$ & 0.15 & 0.25 & 0.60 & \\
$y_i*P(Y=y_i)$ & 0.00 & 39.75 & 120.00 & $E(Y) = \$159.75$\\
$y_i-E(Y)$ & -159.75 & -0.75 & 40.25 & \\
$(y_i-E(Y))^2$ & 25520.06 & 0.56 & 1620.06 & \\
$(y_i-E(Y))^2*P(Y)$ & 3828.0 & 0.1 & 972.0 & $Var(Y) \approx 4800$ \\
   \hline
\end{tabular}
\end{center}}.
\begin{enumerate}
\item[(a)] What proportion of students don't buy either book? Again assume no students buy the supplement without the textbook.
\item[(b)] Let $Y$ represent the revenue from a single student. Write out the probability distribution of $Y$, i.e. a table for each outcome and its associated probability.
\item[(c)] Compute the expected revenue from a single chemistry student. 
\item[(d)] Find the standard deviation to describe the variability associated with the revenue from a single student.
\end{enumerate}
\end{exercise}

%\subsection{Decision trees}

%To be completed later.

\subsection{Linear combinations of random variables}

So far, we have thought of each variable as being a complete story in and of itself. Sometimes it is more appropriate to use a combination of variables. For instance, the amount of time a person spends commuting to work each week can be broken down into several daily commutes. Similarly, the total gain or loss in a stock portfolio is only the sum of the gains and losses in its components.

\begin{example}{John travels to work five days a week. We will use $X_1$ to represent his travel time on Monday, $X_2$ to represent his travel time on Tuesday, and so on. Write an equation using $X_1$, ..., $X_5$ that represents his travel time for the week, denoted by $W$.}
His total weekly travel time is the sum of the five daily values:
$$ W = X_1 + X_2 + X_3 + X_4 + X_5 $$
Breaking the weekly travel time into pieces provides a framework for understanding each source of randomness and how best to model them.
\end{example}

\begin{example}{It takes John an average of 18 minutes each day to commute. What would you expect his average commute time to be for the week?}
We were told that the average (i.e. expected value) of the commute time is 18 minutes per day: $E(X_i) = 18$. To get the expected time for the sum of the five days, we can add up the expected time for each individual day:
\begin{align*}
E(W) &= E(X_1 + X_2 + X_3 + X_4 + X_5) \\
	&= E(X_1) + E(X_2) + E(X_3) + E(X_4) + E(X_5) \\
	&= 18 + 18 + 18 + 18 + 18 = 90\text{ minutes}
\end{align*}
The expectation of the total time is equal to the sum of the expected individual times. More generally, the expectation of a sum of random variables is always the sum of the expectation for each random variable.
%John should expect to spend about 90 minutes (1.5 hours) commuting each week.
\end{example}

\begin{exercise} \label{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction}
Elena is selling a TV at a cash auction and also intends to buy a toaster oven in the auction. If $X$ represents the profit for selling the TV and $Y$ represents the cost of the toaster oven, write an equation that represents the net change in Elena's cash. Answer in the footnote\footnote{She will make $X$ dollars on the TV but spend $Y$ dollars on the toaster oven: $X-Y$.}.
\end{exercise}

\begin{exercise}
Based on past auctions, Elena figures she should expect to make about \$175 on the TV and pay about \$23 for the toaster oven. In total, how much should she expect to make or spend? Answer in the footnote\footnote{$E(X-Y) = E(X) - E(Y) = 175 - 23 = \$152$. She should expect to make about \$152.}.
\end{exercise}

\begin{exercise} \label{explainWhyThereIsUncertaintyInTheSum}
Would you be surprised if John's weekly commute wasn't exactly 90 minutes or if Elena didn't make exactly \$152? Explain.
\end{exercise}

Two important concepts concerning combinations of random variables have so far been introduced. First, a final value can sometimes be described as the sum of its parts in an equation. Second, intuition suggests that putting the individual average values into this equation gives the average value we would expect in total. This second point needs clarification -- it is guaranteed to be true in what are called \emph{linear combinations of random variables}.

A \term{linear combination} of two random variables $X$ and $Y$ is a fancy phrase to describe a combination
$$ aX + bY$$
where $a$ and $b$ are some fixed and known numbers. For John's commute time, there were five random variables -- one for each work day -- and each random variable could be written as having a fixed coefficient of 1:
$$ 1*X_1 + 1*X_2 + 1*X_3 + 1*X_4 + 1*X_5 $$
For Elena's net gain or loss, the $X$ random variable had a coefficient of +1 and the $Y$ random variable had a coefficient of -1.

When considering the average of a linear combination of random variables, it is safe to plug in the mean of each random variable and then compute the final result. For a few examples of nonlinear combinations of random variables -- cases where we cannot simply plug in the means -- are provided in the footnote\footnote{If $X$ and $Y$ are random variables, consider the following combinations: $X^{1+Y}$, $X*Y$, $X/Y$. In such cases, plugging in the average value for each random variable and computing the result will not generally lead to an accurate average value for the end result.}.

\begin{termBox}{\tBoxTitle{Linear combinations of random variables and the average result}
If $X$ and $Y$ are random variables, then a linear combination of the random variables is given by
\begin{align}\label{linComboOfRandomVariablesXAndY}
aX + bY
\end{align}
where $a$ and $b$ are some fixed numbers. To compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result:
\begin{align*}
a\mu_{X} + b\mu_{Y}
\end{align*}
Recall that the expected value is the same as the mean (e.g. $E(X) = \mu_X$).}
\end{termBox}

\begin{example}{Leonard has invested \$6000 in Google Inc. (stock ticker: GOOG) and \$2000 in Exxon Mobil Corp. (XOM). If $X$ represents the change in Google's stock next month and $Y$ represents the change in Exxon Mobil stock next month, write an equation that describes how much money will be made or lost in Leonard's stocks for the month.}
For simplicity, we will suppose $X$ and $Y$ are not in percents but are in decimal form (e.g. if Google's stock increases 1\%, then $X=0.01$; or if it loses 1\%, then $X=-0.01$). Then we can write an equation for Leonard's gain as
\begin{align*}
\$6000*X + \$2000*Y
\end{align*}
If we plug in the change in the stock value for $X$ and $Y$, this equation gives the change in value of Leonard's stock portfolio for the month. A positive value represents a gain, and a negative value represents a loss.
\end{example}

\begin{exercise}\label{expectedChangeInLeonardsStockPortfolio}
Suppose Google and Exxon Mobil stocks have recently been rising 1.1\% and 0.56\% per month, respectively. Compute the expected change in Leonard's stock portfolio for next month.
\end{exercise}

\begin{exercise}
You should have found that Leonard expects a positive gain in Exercise~\ref{expectedChangeInLeonardsStockPortfolio}. However, would you be surprised if he actually had a loss this month?
\end{exercise}

%Thus far, we have only considered how to write linear combinations of random variables and compute the expected value (e.g. the average outcome) of the entire expression using the averages for the individual random variables. Next we consider the variability in the outcome using the variance and standard deviation.

\subsection{Variability in linear combinations of random variables}

Quantifying the average outcome from a linear combination of random variables is helpful, but it is also important to have some sense of the uncertainty associated with the total outcome of that combination of random variables. The expected net gain or loss of Leonard's stock portfolio was considered in Exercise~\ref{expectedChangeInLeonardsStockPortfolio}. However, there was no quantitative discussion of the volatility of this portfolio. For instance, while the average monthly gain might be about \$75 according to the data, that gain is not guaranteed. Figure~\ref{changeInLeonardsStockPortfolioFor36Months} shows the monthly changes in a portfolio like Leonard's during the 36 months from 2008 to 2010. The gains and losses vary widely, and quantifying these fluctuations is important when investing in stocks.
\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{02/figures/changeInLeonardsStockPortfolioFor36Months/changeInLeonardsStockPortfolioFor36Months}
\caption{The change in a portfolio like Leonard's for the 36 months from 2008 to 2010, where \$6000 is in Google's stock and \$2000 is in Exxon Mobil's.}
\label{changeInLeonardsStockPortfolioFor36Months}
\end{figure}

Just as we have done in many previous cases, we use the variance and standard deviation to describe the uncertainty associated with Leonard's monthly returns. To do so, the variances of each stock's monthly return will be useful, and these are shown in Table~\ref{sumStatOfGOOGXOM}. The stocks' returns are nearly independent.
\begin{table}
\centering
\begin{tabular}{lrrr}
\hline
	& Mean ($\bar{x}$) & Standard deviation ($s$) & Variance ($s^2$) \\
\hline
GOOG & 0.0110	& 0.1050					&	0.0110	\\
XOM & 0.0056		& 0.0503					&	0.0025	\\
\hline
\end{tabular}
\caption{The mean, standard deviation, and variance of the GOOG and XOM stocks. These statistics were estimated from historical stock data, so notation used for sample statistics has been used.}
\label{sumStatOfGOOGXOM}
\end{table}

Here we use an equation from probability theory to describe the uncertainty of Leonard's monthly returns; we leave the proof of this method to a dedicated probability course. The variance of a linear combination of random variables can be computed by plugging in the variances of the individual random variables and squaring the coefficients of the random variables:
\begin{align*}
Var(aX + bY) = a^2*Var(X) + b^2*Var(Y)
\end{align*}
It is important to note that this equality assumes the random variables are independent; if independence doesn't hold, then more advanced methods are necessary. This equation can be used to compute the variance of Leonard's monthly return:
\begin{align*}
Var(6000*X + 2000*Y)
	&= 6000^2*Var(X) + 2000^2*Var(Y) \\
	&= 36,000,000*0.0110 + 4,000,000*0.0025 \\
	&= 405,796
\end{align*}
To get the standard deviation of the monthly variability, take the square root of the variance: $\sqrt{405,796} = \$637$. While an average monthly return of \$79 on a \$6000 investment is nothing to scoff at, the monthly returns are so volatile that Leonard should not expect this income to be very stable.

\begin{termBox}{\tBoxTitle{Variability of linear combinations of random variables}
The variance of a linear combination of random variables may be computed by squaring the constants, substituting in the variances for the random variables, and computing the result:
\begin{align*}
Var(aX + bY) = a^2*Var(X) + b^2*Var(Y)
\end{align*}
This equation is valid as long as the random variables are independent of each other. The standard deviation of the linear combination may be found by taking the square root of the variance result.}
\end{termBox}

\begin{example}{Suppose John's daily commute has a standard deviation of 4 minutes. What is the uncertainty in his total commute time for the week?} \label{sdOfJohnsCommuteWeeklyTime}
The expression for John's commute time was
\begin{align*}
X_1 + X_2 + X_3 + X_4 + X_5
\end{align*}
Each coefficient is 1, and the variance of each day's time is $4^2=16$. Thus, the variance of the total weekly commute time is
\begin{align*}
&\text{variance }= 1^2 * 16 + 1^2 * 16 + 1^2 * 16 + 1^2 * 16 + 1^2 * 16 = 5*16 = 80 \\
&\text{standard deviation } = \sqrt{\text{variance}} = \sqrt{80} = 8.94
\end{align*}
The standard deviation for John's weekly work commute time is about 9 minutes.
\end{example}

\begin{exercise}
The computation in Example~\ref{sdOfJohnsCommuteWeeklyTime} relied on an important assumption: the commute time for each day is independent of the time on other days of that week. Do you think this is valid? Explain.
\end{exercise}

\begin{exercise}\label{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuctionVariability}
Consider Elena's two auction's from Exercise~\ref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction} on page~\pageref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction}. Suppose these auctions are approximately independent and the variability in auction prices associated with the TV and toaster oven can be described using standard deviations of \$25 and \$8. Compute the standard deviation of Elena's net gain. Answer in the footnote\footnote{The equation for Elena can be written as
\begin{align*}
(1)*X + (-1)*Y
\end{align*}
The variances of $X$ and $Y$ are 625 and 64. We square the coefficients and plug in the variances:
\begin{align*}
(1)^2*Var(X) + (-1)^2*Var(Y) = 1*625 + 1*64 = 689
\end{align*}
The variance of the linear combination is 689, and the standard deviation is the square root of 689: about \$26.25.}.
\end{exercise}

Consider again Exercise~\ref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuctionVariability}. The negative coefficient for $Y$ in the linear combination was eliminated when we squared the coefficients. This generally holds true: negatives in a linear combination will have no impact on the variability computed for a linear combination, but they do impact the expected value computations.



%\subsection{Conditional expectation}

%To be completed later.

